{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #解压数据集\n",
    "# import os\n",
    "# import shutil\n",
    "# if os.path.exists('data/data9048/images-6.4'):\n",
    "#     shutil.rmtree('data/data9048/images-6.4')\n",
    "# !cd /home/aistudio/data/data9048/ && unzip -q images-6.4.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 解压数据集2\n",
    "# import os\n",
    "# import shutil\n",
    "# if os.path.exists('data/data21566/images-2.17_1'):\n",
    "#     shutil.rmtree('data/data21566/images-2.17_1')\n",
    "# !cd /home/aistudio/data/data21566/ && unzip -q image_2.20.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据清洗\n",
    "import os.path\n",
    "import os\n",
    "from os import listdir\n",
    "import imghdr\n",
    "from PIL import Image \n",
    "import shutil \n",
    "import cv2\n",
    "from PIL import ImageFile\n",
    "\n",
    "path = 'data/data21566/images-2.17_1'\n",
    "# path = 'data/data9048/images-6.4'\n",
    "\n",
    "# def removeimage(image_path):\n",
    "#     n = 0\n",
    "#     for root, dirs, files in os.walk(image_path):\n",
    "#         for name in files:\n",
    "#             if((name.endswith(\".jpg\"))==0):\n",
    "#                 print(\"删除图片：%s\"%name)\n",
    "#                 os.remove(os.path.join(root, name))\n",
    "#             elif(cv2.imread(os.path.join(root, name))) is None:\n",
    "#                 print(name)\n",
    "#                 os.remove(os.path.join(root, name))  # 删除有问题的图片\n",
    "#             else:\n",
    "#                 img_path = os.path.join(root, name)\n",
    "#                 image = Image.open(img_path)              # 打开特定一张图片\n",
    "#                 ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#                 if image.mode == \"RGBA\":\n",
    "#                   image = image.convert('RGB')\n",
    "#                 if image.mode == \"P\":\n",
    "#                   image = image.convert('RGB')\n",
    "#                 image = image.resize(image_size)            # 设置需要转换的图片大小\n",
    "#                 os.remove(img_path)\n",
    "#                 image.save(img_path)\n",
    "#                 n += 1\n",
    "#     print(\"%s的图片尺寸统一完毕，共有%d张\"%(root,n))\n",
    "#     return n\n",
    "\n",
    "# def cleanfile(file_path):\n",
    "#     sum = 0\n",
    "#     for name in os.listdir(file_path):\n",
    "#         image_path = file_path +'/'+name\n",
    "#         print (\">>>文件夹：%s\"%image_path)\n",
    "#         if (name == \".DS_Store\"):\n",
    "#             print (\"删除文件%s\"%name)\n",
    "#             os.remove(image_path) #删除神奇的文件\n",
    "#             continue;\n",
    "#         if (name == \".ipynb_checkpoints\"):\n",
    "#             print (\"删除文件%s\"%name)\n",
    "#             os.remove(image_path) #删除神奇的文件\n",
    "#             continue;\n",
    "#         sum = sum + removeimage(image_path) \n",
    "    \n",
    "#     print('共有%d张图片'%sum)\n",
    "\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     image_size = (224, 224)\n",
    "#     #数据集1：7种\n",
    "#     #path = '/home/aistudio/data/data9048/images-6.4'\n",
    "#     # #数据集2: 19种\n",
    "#     # path = 'data/data21566/images-2.17_1'\n",
    "#     cleanfile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 预处理数据，将其转化为标准格式。同时将数据拆分成两份，以便训练和计算预估准确率 \r\n",
    "# import codecs   \r\n",
    "# import random   \r\n",
    "   \r\n",
    "# train_ratio = 4.0/ 5   \r\n",
    "   \r\n",
    "# all_file_dir = path   \r\n",
    "# class_list = [c for c in os.listdir(all_file_dir) if os.path.isdir(os.path.join(all_file_dir, c)) and not c.endswith('Set') and not c.startswith('.')]   \r\n",
    "# class_list.sort() \r\n",
    "# print(class_list)   \r\n",
    "# train_image_dir = os.path.join(all_file_dir, \"trainImageSet\")   \r\n",
    "# if not os.path.exists(train_image_dir):   \r\n",
    "#     os.makedirs(train_image_dir)   \r\n",
    "       \r\n",
    "# eval_image_dir = os.path.join(all_file_dir, \"evalImageSet\")   \r\n",
    "# if not os.path.exists(eval_image_dir):   \r\n",
    "#     os.makedirs(eval_image_dir)   \r\n",
    "   \r\n",
    "# train_file = codecs.open(os.path.join(all_file_dir, \"train.txt\"), 'w')   \r\n",
    "# eval_file = codecs.open(os.path.join(all_file_dir, \"eval.txt\"), 'w')   \r\n",
    "   \r\n",
    "# with codecs.open(os.path.join(all_file_dir, \"label_list.txt\"), \"w\") as label_list:   \r\n",
    "#     label_id = 0   \r\n",
    "#     for class_dir in class_list:   \r\n",
    "#         label_list.write(\"{0}\\t{1}\\n\".format(label_id, class_dir))   \r\n",
    "#         image_path_pre = os.path.join(all_file_dir, class_dir)   \r\n",
    "#         for file in os.listdir(image_path_pre):   \r\n",
    "#             try:   \r\n",
    "#                 img = Image.open(os.path.join(image_path_pre, file))   \r\n",
    "#                 if random.uniform(0, 1) <= train_ratio:   \r\n",
    "#                     shutil.copyfile(os.path.join(image_path_pre, file), os.path.join(train_image_dir, file))   \r\n",
    "#                     train_file.write(\"{0}\\t{1}\\n\".format(os.path.join(train_image_dir, file), label_id))   \r\n",
    "#                 else:   \r\n",
    "#                     shutil.copyfile(os.path.join(image_path_pre, file), os.path.join(eval_image_dir, file))   \r\n",
    "#                     eval_file.write(\"{0}\\t{1}\\n\".format(os.path.join(eval_image_dir, file), label_id))   \r\n",
    "#             except Exception as e:   \r\n",
    "#                 pass   \r\n",
    "#                 # 存在一些文件打不开，此处需要稍作清洗   \r\n",
    "#         label_id += 1   \r\n",
    "               \r\n",
    "# train_file.close()   \r\n",
    "# eval_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amanita', 'Armillaria_mellea', 'Boletus', 'Cantharellus_cibarius', 'Collybia', 'Ganoderma', 'Laccaria', 'Lactarius', 'Lentinus_edodes', 'Morchella', 'Ophiocordyceps_sinensis', 'Pleurotus_citrinopileatus', 'Ramaria', 'Russula', 'Saarcodon_imbricatum', 'Thelephora_ganbajun', 'Trichloma_matsutake', 'Tuber']\n"
     ]
    }
   ],
   "source": [
    "import codecs   \n",
    "import random  \n",
    "\n",
    "train_ratio = 6.0/ 10  \n",
    "test_ratio = 8.0/ 10\n",
    "path = 'data/data21566/images-2.17_1'\n",
    "# path = 'data/data9048/images-6.4'\n",
    "\n",
    "all_file_dir = path   \n",
    "class_list = [c for c in os.listdir(all_file_dir) if os.path.isdir(os.path.join(all_file_dir, c)) and not c.endswith('Set') and not c.startswith('.')]   \n",
    "class_list.sort() \n",
    "print(class_list)   \n",
    "train_image_dir = os.path.join(all_file_dir, \"trainImageSet\")   \n",
    "if not os.path.exists(train_image_dir):   \n",
    "    os.makedirs(train_image_dir)   \n",
    "       \n",
    "eval_image_dir = os.path.join(all_file_dir, \"evalImageSet\")   \n",
    "if not os.path.exists(eval_image_dir):   \n",
    "    os.makedirs(eval_image_dir) \n",
    "\n",
    "test_image_dir = os.path.join(all_file_dir, \"testImageSet\")   \n",
    "if not os.path.exists(test_image_dir):   \n",
    "    os.makedirs(test_image_dir)\n",
    "   \n",
    "train_file = codecs.open(os.path.join(all_file_dir, \"train.txt\"), 'w')   \n",
    "eval_file = codecs.open(os.path.join(all_file_dir, \"eval.txt\"), 'w')\n",
    "test_file = codecs.open(os.path.join(all_file_dir, \"test.txt\"), 'w')\n",
    "   \n",
    "with codecs.open(os.path.join(all_file_dir, \"label_list.txt\"), \"w\") as label_list:   \n",
    "    label_id = 0   \n",
    "    for class_dir in class_list:   \n",
    "        label_list.write(\"{0}\\t{1}\\n\".format(label_id, class_dir))   \n",
    "        image_path_pre = os.path.join(all_file_dir, class_dir)   \n",
    "        for file in os.listdir(image_path_pre):   \n",
    "            try:   \n",
    "                img = Image.open(os.path.join(image_path_pre, file))\n",
    "                rand = random.uniform(0, 1)\n",
    "                if  rand <= train_ratio:   \n",
    "                    shutil.copyfile(os.path.join(image_path_pre, file), os.path.join(train_image_dir, file))   \n",
    "                    train_file.write(\"{0}\\t{1}\\n\".format(os.path.join(train_image_dir, file), label_id))   \n",
    "                elif rand >= test_ratio:\n",
    "                    shutil.copyfile(os.path.join(image_path_pre, file), os.path.join(test_image_dir, file))   \n",
    "                    test_file.write(\"{0}\\t{1}\\n\".format(os.path.join(test_image_dir, file), label_id))\n",
    "                else:   \n",
    "                    shutil.copyfile(os.path.join(image_path_pre, file), os.path.join(eval_image_dir, file))   \n",
    "                    eval_file.write(\"{0}\\t{1}\\n\".format(os.path.join(eval_image_dir, file), label_id))   \n",
    "            except Exception as e:   \n",
    "                pass   \n",
    "                # 存在一些文件打不开，此处需要稍作清洗   \n",
    "        label_id += 1   \n",
    "               \n",
    "train_file.close()   \n",
    "eval_file.close()\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*- \r\n",
    "\"\"\" \r\n",
    "训练常用视觉基础网络，用于分类任务 \r\n",
    "需要将训练图片，类别文件 label_list.txt 放置在同一个文件夹下 \r\n",
    "程序会先读取 train.txt 文件获取类别数和图片数量 \r\n",
    "\"\"\" \r\n",
    "from __future__ import absolute_import \r\n",
    "from __future__ import division \r\n",
    "from __future__ import print_function \r\n",
    "import os\r\n",
    "os.environ['FLAGS_eager_delete_tensor_gb'] = '0'\r\n",
    "os.environ['FLAGS_fraction_of_gpu_memory_to_use'] = '0.75'\r\n",
    "import numpy as np \r\n",
    "import time \r\n",
    "import math \r\n",
    "import paddle \r\n",
    "import paddle.fluid as fluid\r\n",
    "from visualdl import LogWriter\r\n",
    "import codecs \r\n",
    "import logging \r\n",
    "import pandas as pd\r\n",
    "from sklearn.utils import shuffle\r\n",
    " \r\n",
    "from paddle.fluid.initializer import MSRA \r\n",
    "from paddle.fluid.initializer import Uniform \r\n",
    "from paddle.fluid.param_attr import ParamAttr \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from PIL import Image \r\n",
    "from PIL import ImageEnhance \r\n",
    " \r\n",
    "train_parameters = {    \r\n",
    "    \"input_size\": [3, 224, 224],\r\n",
    "    \"class_dim\": -1,  # 分类数，会在初始化自定义 reader 的时候获得    \r\n",
    "    \"image_count\": -1,  # 训练图片数量，会在初始化自定义 reader 的时候获得    \r\n",
    "    \"label_dict\": {},\r\n",
    "    \"data_dir\": path,  # 训练数据存储地址    \r\n",
    "    \"train_file_list\": \"train.txt\",\r\n",
    "    \"label_file\": \"label_list.txt\",\r\n",
    "    \"save_freeze_dir\": \"./freeze-model\",\r\n",
    "    \"save_test_dir\": \"./test-model\",\r\n",
    "    \"save_persistable_dir\": \"./persistable-params\",\r\n",
    "    \"continue_train\": False,        # 是否接着上一次保存的参数接着训练，优先级高于预训练模型    \r\n",
    "    \"pretrained\": False,            # 是否使用预训练的模型    \r\n",
    "    \"pretrained_dir\": \"data/data9048/DenseNet_pretrained\",\r\n",
    "    \"mode\": \"train\",\r\n",
    "    \"num_epochs\": 5,\r\n",
    "    \"train_batch_size\": 32,\r\n",
    "    \"mean_rgb\": [127.5, 127.5, 127.5],  # 常用图片的三通道均值，通常来说需要先对训练数据做统计，此处仅取中间值    \r\n",
    "    \"use_gpu\": True,\r\n",
    "    \"dropout_prob\": 0.2,\r\n",
    "    \"dropout_seed\": None,\r\n",
    "    \"image_enhance_strategy\": {  # 图像增强相关策略    \r\n",
    "        \"need_distort\": False,  # 是否启用图像颜色增强    \r\n",
    "        \"need_rotate\":True,   # 是否需要增加随机角度    \r\n",
    "        \"need_crop\":True,      # 是否要增加裁剪    \r\n",
    "        \"need_flip\": False,      # 是否要增加水平随机翻转    \r\n",
    "        \"hue_prob\": 0.5,\r\n",
    "        \"hue_delta\": 18,\r\n",
    "        \"contrast_prob\": 0.5,\r\n",
    "        \"contrast_delta\": 0.5,\r\n",
    "        \"saturation_prob\": 0.5,\r\n",
    "        \"saturation_delta\": 0.5,\r\n",
    "        \"brightness_prob\": 0.5,\r\n",
    "        \"brightness_delta\": 0.125,\r\n",
    "        \"rotate_prob\": 0.5,\r\n",
    "        \"rotate_range\": 14\r\n",
    "    },\r\n",
    "    \"early_stop\": {\r\n",
    "        \"sample_frequency\": 50,\r\n",
    "        \"successive_limit\": 5,\r\n",
    "        \"good_acc1\": 0.95\r\n",
    "    },\r\n",
    "    \"rsm_strategy\": {\r\n",
    "        \"learning_rate\": 0.001,\r\n",
    "        \"lr_epochs\": [20, 40, 60, 80, 100],\r\n",
    "        \"lr_decay\": [1, 0.5, 0.25, 0.1, 0.05, 0.01]\r\n",
    "    }\r\n",
    "}\r\n",
    " \r\n",
    " \r\n",
    "\r\n",
    "\r\n",
    " \r\n",
    "def init_log_config(): \r\n",
    "    \"\"\" \r\n",
    "    初始化日志相关配置 \r\n",
    "    :return: \r\n",
    "    \"\"\" \r\n",
    "    global logger \r\n",
    "    logger = logging.getLogger() \r\n",
    "    logger.setLevel(logging.INFO) \r\n",
    "    log_path = os.path.join(os.getcwd(), 'logs') \r\n",
    "    if not os.path.exists(log_path): \r\n",
    "        os.makedirs(log_path) \r\n",
    "    log_name = os.path.join(log_path, 'train.log') \r\n",
    "    sh = logging.StreamHandler() \r\n",
    "    fh = logging.FileHandler(log_name, mode='w') \r\n",
    "    fh.setLevel(logging.DEBUG) \r\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\") \r\n",
    "    fh.setFormatter(formatter) \r\n",
    "    sh.setFormatter(formatter) \r\n",
    "    logger.addHandler(sh) \r\n",
    "    logger.addHandler(fh)\r\n",
    "    \r\n",
    "        \r\n",
    " \r\n",
    "def init_train_parameters(): \r\n",
    "    \"\"\" \r\n",
    "    初始化训练参数，主要是初始化图片数量，类别数 \r\n",
    "    :return: \r\n",
    "    \"\"\" \r\n",
    "    train_file_list = os.path.join(train_parameters['data_dir'], train_parameters['train_file_list']) \r\n",
    "    label_list = os.path.join(train_parameters['data_dir'], train_parameters['label_file']) \r\n",
    "    index = 0 \r\n",
    "    with codecs.open(label_list, encoding='utf-8') as flist: \r\n",
    "        lines = [line.strip() for line in flist] \r\n",
    "        for line in lines: \r\n",
    "            parts = line.strip().split() \r\n",
    "            train_parameters['label_dict'][parts[1]] = int(parts[0]) \r\n",
    "            index += 1 \r\n",
    "        train_parameters['class_dim'] = index \r\n",
    "    with codecs.open(train_file_list, encoding='utf-8') as flist: \r\n",
    "        lines = [line.strip() for line in flist] \r\n",
    "        train_parameters['image_count'] = len(lines) \r\n",
    " \r\n",
    "def resize_img(img, target_size): \r\n",
    "    \"\"\" \r\n",
    "    强制缩放图片 \r\n",
    "    :param img: \r\n",
    "    :param target_size: \r\n",
    "    :return: \r\n",
    "    \"\"\" \r\n",
    "    target_size = input_size \r\n",
    "    img = img.resize((target_size[1], target_size[2]), Image.BILINEAR) \r\n",
    "    return img \r\n",
    " \r\n",
    " \r\n",
    "def random_crop(img, scale=[0.08, 1.0], ratio=[3. / 4., 4. / 3.]): \r\n",
    "    aspect_ratio = math.sqrt(np.random.uniform(*ratio)) \r\n",
    "    w = 1. * aspect_ratio \r\n",
    "    h = 1. / aspect_ratio \r\n",
    " \r\n",
    "    bound = min((float(img.size[0]) / img.size[1]) / (w**2), \r\n",
    "                (float(img.size[1]) / img.size[0]) / (h**2)) \r\n",
    "    scale_max = min(scale[1], bound) \r\n",
    "    scale_min = min(scale[0], bound) \r\n",
    " \r\n",
    "    target_area = img.size[0] * img.size[1] * np.random.uniform(scale_min, \r\n",
    "                                                                scale_max) \r\n",
    "    target_size = math.sqrt(target_area) \r\n",
    "    w = int(target_size * w) \r\n",
    "    h = int(target_size * h) \r\n",
    " \r\n",
    "    i = np.random.randint(0, img.size[0] - w + 1) \r\n",
    "    j = np.random.randint(0, img.size[1] - h + 1) \r\n",
    " \r\n",
    "    img = img.crop((i, j, i + w, j + h)) \r\n",
    "    img = img.resize((train_parameters['input_size'][1], train_parameters['input_size'][2]), Image.BILINEAR) \r\n",
    "    return img \r\n",
    " \r\n",
    " \r\n",
    "def rotate_image(img): \r\n",
    "    \"\"\" \r\n",
    "    图像增强，增加随机旋转角度 \r\n",
    "    \"\"\" \r\n",
    "    prob = np.random.uniform(0, 1) \r\n",
    "    if prob < train_parameters['image_enhance_strategy']['rotate_prob']: \r\n",
    "        range = train_parameters['image_enhance_strategy']['rotate_range']\r\n",
    "        angle = np.random.randint(-range, range) \r\n",
    "        img = img.rotate(angle) \r\n",
    "    return img \r\n",
    " \r\n",
    " \r\n",
    "def random_brightness(img): \r\n",
    "    \"\"\" \r\n",
    "    图像增强，亮度调整 \r\n",
    "    :param img: \r\n",
    "    :return: \r\n",
    "    \"\"\" \r\n",
    "    prob = np.random.uniform(0, 1) \r\n",
    "    if prob < train_parameters['image_enhance_strategy']['brightness_prob']: \r\n",
    "        brightness_delta = train_parameters['image_enhance_strategy']['brightness_delta'] \r\n",
    "        delta = np.random.uniform(-brightness_delta, brightness_delta) + 1 \r\n",
    "        img = ImageEnhance.Brightness(img).enhance(delta) \r\n",
    "    return img \r\n",
    " \r\n",
    " \r\n",
    "def random_contrast(img): \r\n",
    "    \"\"\" \r\n",
    "    图像增强，对比度调整 \r\n",
    "    :param img: \r\n",
    "    :return: \r\n",
    "    \"\"\" \r\n",
    "    prob = np.random.uniform(0, 1) \r\n",
    "    if prob < train_parameters['image_enhance_strategy']['contrast_prob']: \r\n",
    "        contrast_delta = train_parameters['image_enhance_strategy']['contrast_delta'] \r\n",
    "        delta = np.random.uniform(-contrast_delta, contrast_delta) + 1 \r\n",
    "        img = ImageEnhance.Contrast(img).enhance(delta) \r\n",
    "    return img \r\n",
    " \r\n",
    " \r\n",
    "def random_saturation(img): \r\n",
    "    \"\"\" \r\n",
    "    图像增强，饱和度调整 \r\n",
    "    :param img: \r\n",
    "    :return: \r\n",
    "    \"\"\" \r\n",
    "    prob = np.random.uniform(0, 1) \r\n",
    "    if prob < train_parameters['image_enhance_strategy']['saturation_prob']: \r\n",
    "        saturation_delta = train_parameters['image_enhance_strategy']['saturation_delta'] \r\n",
    "        delta = np.random.uniform(-saturation_delta, saturation_delta) + 1 \r\n",
    "        img = ImageEnhance.Color(img).enhance(delta) \r\n",
    "    return img \r\n",
    " \r\n",
    " \r\n",
    "def random_hue(img): \r\n",
    "    \"\"\" \r\n",
    "    图像增强，色度调整 \r\n",
    "    :param img: \r\n",
    "    :return: \r\n",
    "    \"\"\" \r\n",
    "    prob = np.random.uniform(0, 1) \r\n",
    "    if prob < train_parameters['image_enhance_strategy']['hue_prob']: \r\n",
    "        hue_delta = train_parameters['image_enhance_strategy']['hue_delta'] \r\n",
    "        delta = np.random.uniform(-hue_delta, hue_delta) \r\n",
    "        img_hsv = np.array(img.convert('HSV')) \r\n",
    "        img_hsv[:, :, 0] = img_hsv[:, :, 0] + delta \r\n",
    "        img = Image.fromarray(img_hsv, mode='HSV').convert('RGB') \r\n",
    "    return img \r\n",
    " \r\n",
    " \r\n",
    "def distort_color(img): \r\n",
    "    \"\"\" \r\n",
    "    概率的图像增强 \r\n",
    "    :param img: \r\n",
    "    :return: \r\n",
    "    \"\"\" \r\n",
    "    prob = np.random.uniform(0, 1) \r\n",
    "    # Apply different distort order \r\n",
    "    if prob < 0.35: \r\n",
    "        img = random_brightness(img) \r\n",
    "        img = random_contrast(img) \r\n",
    "        img = random_saturation(img) \r\n",
    "        img = random_hue(img) \r\n",
    "    elif prob < 0.7: \r\n",
    "        img = random_brightness(img) \r\n",
    "        img = random_saturation(img) \r\n",
    "        img = random_hue(img) \r\n",
    "        img = random_contrast(img) \r\n",
    "    return img \r\n",
    " \r\n",
    "def image_shuffle(): \r\n",
    "    import pandas as pd\r\n",
    "    from sklearn.utils import shuffle\r\n",
    "    data = pd.read_csv(os.path.join(path, 'train.txt'))\r\n",
    "    dt = shuffle(data)\r\n",
    "    #print(dt[1:3])\r\n",
    "    dt.to_csv(os.path.join(path, 'train.txt'),header=None, index=False,)\r\n",
    " \r\n",
    "def custom_image_reader(file_list, data_dir, mode): \r\n",
    "    \"\"\" \r\n",
    "    自定义用户图片读取器，先初始化图片种类，数量 \r\n",
    "    :param file_list: \r\n",
    "    :param data_dir: \r\n",
    "    :param mode: \r\n",
    "    :return: \r\n",
    "    \"\"\" \r\n",
    "    with codecs.open(file_list) as flist: \r\n",
    "        lines = [line.strip() for line in flist] \r\n",
    " \r\n",
    "    def reader(): \r\n",
    "        np.random.shuffle(lines) \r\n",
    "        for line in lines: \r\n",
    "            if mode == 'train' or mode == 'val': \r\n",
    "                img_path, label = line.split() \r\n",
    "                img = Image.open(img_path) \r\n",
    "                try: \r\n",
    "                    if img.mode != 'RGB': \r\n",
    "                        img = img.convert('RGB') \r\n",
    "                    if train_parameters['image_enhance_strategy']['need_distort'] == True: \r\n",
    "                        img = distort_color(img) \r\n",
    "                    if train_parameters['image_enhance_strategy']['need_rotate'] == True: \r\n",
    "                        img = rotate_image(img) \r\n",
    "                    if train_parameters['image_enhance_strategy']['need_crop'] == True: \r\n",
    "                        img = random_crop(img, train_parameters['input_size']) \r\n",
    "                    if train_parameters['image_enhance_strategy']['need_flip'] == True: \r\n",
    "                        mirror = int(np.random.uniform(0, 2)) \r\n",
    "                        if mirror == 1: \r\n",
    "                            img = img.transpose(Image.FLIP_LEFT_RIGHT) \r\n",
    "                    # HWC--->CHW && normalized \r\n",
    "                    img = np.array(img).astype('float32') \r\n",
    "                    img -= train_parameters['mean_rgb'] \r\n",
    "                    img = img.transpose((2, 0, 1))  # HWC to CHW \r\n",
    "                    img *= 0.007843                 # 像素值归一化 \r\n",
    "                    yield img, int(label) \r\n",
    "                except Exception as e: \r\n",
    "                    pass                            # 以防某些图片读取处理出错，加异常处理 \r\n",
    "            elif mode == 'test': \r\n",
    "                img_path = os.path.join(data_dir, line) \r\n",
    "                img = Image.open(img_path) \r\n",
    "                if img.mode != 'RGB': \r\n",
    "                    img = img.convert('RGB') \r\n",
    "                img = resize_img(img, train_parameters['input_size']) \r\n",
    "                # HWC--->CHW && normalized \r\n",
    "                img = np.array(img).astype('float32') \r\n",
    "                img -= train_parameters['mean_rgb'] \r\n",
    "                img = img.transpose((2, 0, 1))  # HWC to CHW \r\n",
    "                img *= 0.007843  # 像素值归一化 \r\n",
    "                yield img \r\n",
    " \r\n",
    "    return reader \r\n",
    " \r\n",
    " \r\n",
    "def optimizer_rms_setting(): \r\n",
    "    \"\"\" \r\n",
    "    阶梯型的学习率适合比较大规模的训练数据 \r\n",
    "    \"\"\" \r\n",
    "    batch_size = train_parameters[\"train_batch_size\"] \r\n",
    "    iters = train_parameters[\"image_count\"] // batch_size \r\n",
    "    learning_strategy = train_parameters['rsm_strategy'] \r\n",
    "    lr = learning_strategy['learning_rate'] \r\n",
    " \r\n",
    "    boundaries = [i * iters for i in learning_strategy[\"lr_epochs\"]] \r\n",
    "    values = [i * lr for i in learning_strategy[\"lr_decay\"]] \r\n",
    " \r\n",
    "    optimizer = fluid.optimizer.RMSProp( \r\n",
    "        learning_rate=fluid.layers.piecewise_decay(boundaries, values)) \r\n",
    " \r\n",
    "    return optimizer \r\n",
    " \r\n",
    " \r\n",
    "def load_params(exe, program): \r\n",
    "    if train_parameters['continue_train'] and os.path.exists(train_parameters['save_persistable_dir']): \r\n",
    "        logger.info('load params from retrain model') \r\n",
    "        fluid.io.load_persistables(executor=exe, \r\n",
    "                                   dirname=train_parameters['save_persistable_dir'], \r\n",
    "                                   main_program=program) \r\n",
    "    elif train_parameters['pretrained'] and os.path.exists(train_parameters['pretrained_dir']): \r\n",
    "        logger.info('load params from pretrained model') \r\n",
    "        def if_exist(var): \r\n",
    "            return os.path.exists(os.path.join(train_parameters['pretrained_dir'], var.name)) \r\n",
    " \r\n",
    "        fluid.io.load_vars(exe, train_parameters['pretrained_dir'], main_program=program, \r\n",
    "                           predicate=if_exist) \r\n",
    " \r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class ResNet():\n",
    "#     def __init__(self, layers=50):\n",
    "#         self.layers = layers\n",
    "        \n",
    "#     def name(self):\n",
    "#         return 'resnet'\n",
    "\n",
    "#     def net(self, input, class_dim=1000):\n",
    "#         layers = self.layers\n",
    "#         supported_layers = [50, 101, 152]\n",
    "#         assert layers in supported_layers, \\\n",
    "#             \"supported layers are {} but input layer is {}\".format(supported_layers, layers)\n",
    "\n",
    "#         if layers == 50:\n",
    "#             depth = [3, 4, 6, 3]\n",
    "#         elif layers == 101:\n",
    "#             depth = [3, 4, 23, 3]\n",
    "#         elif layers == 152:\n",
    "#             depth = [3, 8, 36, 3]\n",
    "#         num_filters = [64, 128, 256, 512]\n",
    "\n",
    "#         conv = self.conv_bn_layer(\n",
    "#             input=input,\n",
    "#             num_filters=64,\n",
    "#             filter_size=7,\n",
    "#             stride=2,\n",
    "#             act='relu',\n",
    "#             name=\"conv1\")\n",
    "#         conv1 = conv \n",
    "        \n",
    "#         conv = fluid.layers.pool2d(\n",
    "#             input=conv,\n",
    "#             pool_size=3,\n",
    "#             pool_stride=2,\n",
    "#             pool_padding=1,\n",
    "#             pool_type='max')\n",
    "\n",
    "#         for block in range(len(depth)):\n",
    "#             for i in range(depth[block]):\n",
    "#                 if layers in [101, 152] and block == 2:\n",
    "#                     if i == 0:\n",
    "#                         conv_name = \"res\" + str(block + 2) + \"a\"\n",
    "#                     else:\n",
    "#                         conv_name = \"res\" + str(block + 2) + \"b\" + str(i)\n",
    "#                 else:\n",
    "#                     conv_name = \"res\" + str(block + 2) + chr(97 + i)\n",
    "#                 conv = self.bottleneck_block(\n",
    "#                     input=conv,\n",
    "#                     num_filters=num_filters[block],\n",
    "#                     stride=2 if i == 0 and block != 0 else 1,\n",
    "#                     name=conv_name)\n",
    "\n",
    "#         pool = fluid.layers.pool2d(\n",
    "#             input=conv, pool_size=7, pool_type='avg', global_pooling=True)\n",
    "#         stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)\n",
    "#         out = fluid.layers.fc(input=pool,\n",
    "#                               size=class_dim,\n",
    "#                               act='softmax', \n",
    "#                               param_attr=fluid.param_attr.ParamAttr(\n",
    "#                                   name='fc_w',\n",
    "#                                   initializer=fluid.initializer.Uniform(-stdv,\n",
    "#                                                                         stdv)))\n",
    "#         return out, conv1\n",
    "\n",
    "#     def conv_bn_layer(self,\n",
    "#                       input,\n",
    "#                       num_filters,\n",
    "#                       filter_size,\n",
    "#                       stride=1,\n",
    "#                       groups=1,\n",
    "#                       act=None,\n",
    "#                       name=None):\n",
    "#         conv = fluid.layers.conv2d(\n",
    "#             input=input,\n",
    "#             num_filters=num_filters,\n",
    "#             filter_size=filter_size,\n",
    "#             stride=stride,\n",
    "#             padding=(filter_size - 1) // 2,\n",
    "#             groups=groups,\n",
    "#             act=None,\n",
    "#             param_attr=ParamAttr(name=name + \"_weights\"),\n",
    "#             bias_attr=False,\n",
    "#             name=name + '.conv2d.output.1')\n",
    "#         if name == \"conv1\":\n",
    "#             bn_name = \"bn_\" + name\n",
    "#         else:\n",
    "#             bn_name = \"bn\" + name[3:]\n",
    "#         return fluid.layers.batch_norm(\n",
    "#             input=conv,\n",
    "#             act=act,\n",
    "#             name=bn_name + '.output.1',\n",
    "#             param_attr=ParamAttr(name=bn_name + '_scale'),\n",
    "#             bias_attr=ParamAttr(bn_name + '_offset'),\n",
    "#             moving_mean_name=bn_name + '_mean',\n",
    "#             moving_variance_name=bn_name + '_variance', )\n",
    "\n",
    "#     def shortcut(self, input, ch_out, stride, name):\n",
    "#         ch_in = input.shape[1]\n",
    "#         if ch_in != ch_out or stride != 1:\n",
    "#             return self.conv_bn_layer(input, ch_out, 1, stride, name=name)\n",
    "#         else:\n",
    "#             return input\n",
    "\n",
    "#     def bottleneck_block(self, input, num_filters, stride, name):\n",
    "#         conv0 = self.conv_bn_layer(\n",
    "#             input=input,\n",
    "#             num_filters=num_filters,\n",
    "#             filter_size=1,\n",
    "#             act='relu',\n",
    "#             name=name + \"_branch2a\")\n",
    "#         conv1 = self.conv_bn_layer(\n",
    "#             input=conv0,\n",
    "#             num_filters=num_filters,\n",
    "#             filter_size=3,\n",
    "#             stride=stride,\n",
    "#             act='relu',\n",
    "#             name=name + \"_branch2b\")\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             input=conv1,\n",
    "#             num_filters=num_filters * 4,\n",
    "#             filter_size=1,\n",
    "#             act=None,\n",
    "#             name=name + \"_branch2c\")\n",
    "\n",
    "#         short = self.shortcut(\n",
    "#             input, num_filters * 4, stride, name=name + \"_branch1\")\n",
    "\n",
    "#         return fluid.layers.elementwise_add(\n",
    "#             x=short, y=conv2, act='relu', name=name + \".add.output.5\")\n",
    "\n",
    "\n",
    "# class MobileNet():\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "    \n",
    "#     def name(self):\n",
    "#         return 'mobile-net'\n",
    "\n",
    "#     def net(self, input, class_dim=1000, scale=1.0):\n",
    "#         # conv1: 112x112\n",
    "#         input = self.conv_bn_layer(\n",
    "#             input,\n",
    "#             filter_size=3,\n",
    "#             num_filters=int(32 * scale),\n",
    "#             stride=2,\n",
    "#             padding=1)\n",
    "\n",
    "#         conv1 = input\n",
    "        \n",
    "#         # 56x56\n",
    "#         input = self.depthwise_separable(\n",
    "#             input,\n",
    "#             num_filters1=32,\n",
    "#             num_filters2=64,\n",
    "#             num_groups=32,\n",
    "#             stride=1,\n",
    "#             scale=scale)\n",
    "\n",
    "#         input = self.depthwise_separable(\n",
    "#             input,\n",
    "#             num_filters1=64,\n",
    "#             num_filters2=128,\n",
    "#             num_groups=64,\n",
    "#             stride=2,\n",
    "#             scale=scale)\n",
    "\n",
    "#         # 28x28\n",
    "#         input = self.depthwise_separable(\n",
    "#             input,\n",
    "#             num_filters1=128,\n",
    "#             num_filters2=128,\n",
    "#             num_groups=128,\n",
    "#             stride=1,\n",
    "#             scale=scale)\n",
    "\n",
    "#         input = self.depthwise_separable(\n",
    "#             input,\n",
    "#             num_filters1=128,\n",
    "#             num_filters2=256,\n",
    "#             num_groups=128,\n",
    "#             stride=2,\n",
    "#             scale=scale)\n",
    "\n",
    "#         # 14x14\n",
    "#         input = self.depthwise_separable(\n",
    "#             input,\n",
    "#             num_filters1=256,\n",
    "#             num_filters2=256,\n",
    "#             num_groups=256,\n",
    "#             stride=1,\n",
    "#             scale=scale)\n",
    "\n",
    "#         input = self.depthwise_separable(\n",
    "#             input,\n",
    "#             num_filters1=256,\n",
    "#             num_filters2=512,\n",
    "#             num_groups=256,\n",
    "#             stride=2,\n",
    "#             scale=scale)\n",
    "\n",
    "#         # 14x14\n",
    "#         for i in range(5):\n",
    "#             input = self.depthwise_separable(\n",
    "#                 input,\n",
    "#                 num_filters1=512,\n",
    "#                 num_filters2=512,\n",
    "#                 num_groups=512,\n",
    "#                 stride=1,\n",
    "#                 scale=scale)\n",
    "#         module1 = input\n",
    "#         # 7x7\n",
    "#         input = self.depthwise_separable(\n",
    "#             input,\n",
    "#             num_filters1=512,\n",
    "#             num_filters2=1024,\n",
    "#             num_groups=512,\n",
    "#             stride=2,\n",
    "#             scale=scale)\n",
    "\n",
    "#         input = self.depthwise_separable(\n",
    "#             input,\n",
    "#             num_filters1=1024,\n",
    "#             num_filters2=1024,\n",
    "#             num_groups=1024,\n",
    "#             stride=1,\n",
    "#             scale=scale)\n",
    "\n",
    "#         # class_dim x 1\n",
    "#         input = paddle.fluid.layers.conv2d(\n",
    "#             input,\n",
    "#             num_filters=class_dim,\n",
    "#             filter_size=1,\n",
    "#             stride=1)\n",
    "\n",
    "#         pool = fluid.layers.pool2d(\n",
    "#             input=input,\n",
    "#             pool_size=0,\n",
    "#             pool_stride=1,\n",
    "#             pool_type='avg',\n",
    "#             global_pooling=True)\n",
    "\n",
    "#         output = fluid.layers.fc(input=pool,\n",
    "#                               size=class_dim,\n",
    "#                               act='softmax', \n",
    "#                               param_attr=ParamAttr(name='fc_w',initializer=MSRA()))\n",
    "        \n",
    "#         return output, conv1\n",
    "\n",
    "#     def conv_bn_layer(self,\n",
    "#                       input,\n",
    "#                       filter_size,\n",
    "#                       num_filters,\n",
    "#                       stride,\n",
    "#                       padding,\n",
    "#                       num_groups=1,\n",
    "#                       act='relu',\n",
    "#                       use_cudnn=True):\n",
    "#         conv = fluid.layers.conv2d(\n",
    "#             input=input,\n",
    "#             num_filters=num_filters,\n",
    "#             filter_size=filter_size,\n",
    "#             stride=stride,\n",
    "#             padding=padding,\n",
    "#             groups=num_groups,\n",
    "#             act=None,\n",
    "#             use_cudnn=use_cudnn,\n",
    "#             param_attr=ParamAttr(initializer=MSRA()),\n",
    "#             bias_attr=False)\n",
    "#         return fluid.layers.batch_norm(input=conv, act=act)\n",
    "\n",
    "#     def depthwise_separable(self, input, num_filters1, num_filters2, num_groups,\n",
    "#                             stride, scale):\n",
    "#         depthwise_conv = self.conv_bn_layer(\n",
    "#             input=input,\n",
    "#             filter_size=3,\n",
    "#             num_filters=int(num_filters1 * scale),\n",
    "#             stride=stride,\n",
    "#             padding=1,\n",
    "#             num_groups=int(num_groups * scale),\n",
    "#             use_cudnn=True)\n",
    "\n",
    "#         pointwise_conv = self.conv_bn_layer(\n",
    "#             input=depthwise_conv,\n",
    "#             filter_size=1,\n",
    "#             num_filters=int(num_filters2 * scale),\n",
    "#             stride=1,\n",
    "#             padding=0)\n",
    "#         return pointwise_conv\n",
    "\n",
    "\n",
    "# class VGGNet():\n",
    "#     def __init__(self, layers=16):\n",
    "#         self.layers = layers\n",
    "        \n",
    "#     def name(self):\n",
    "#         return 'vgg-net'\n",
    "\n",
    "#     def net(self, input, class_dim=1000):\n",
    "#         layers = self.layers\n",
    "#         vgg_spec = {\n",
    "#             11: ([1, 1, 2, 2, 2]),\n",
    "#             13: ([2, 2, 2, 2, 2]),\n",
    "#             16: ([2, 2, 3, 3, 3]),\n",
    "#             19: ([2, 2, 4, 4, 4])\n",
    "#         }\n",
    "#         assert layers in vgg_spec.keys(), \\\n",
    "#             \"supported layers are {} but input layer is {}\".format(vgg_spec.keys(), layers)\n",
    "\n",
    "#         nums = vgg_spec[layers]\n",
    "#         conv1 = self.conv_block(input, 64, nums[0])\n",
    "#         conv2 = self.conv_block(conv1, 128, nums[1])\n",
    "#         conv3 = self.conv_block(conv2, 256, nums[2])\n",
    "#         conv4 = self.conv_block(conv3, 512, nums[3])\n",
    "#         conv5 = self.conv_block(conv4, 512, nums[4])\n",
    "\n",
    "#         fc_dim = 4096\n",
    "#         fc1 = fluid.layers.fc(\n",
    "#             input=conv5,\n",
    "#             size=fc_dim,\n",
    "#             act='relu',\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Normal(scale=0.005)),\n",
    "#             bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Constant(value=0.1)))\n",
    "#         fc1 = fluid.layers.dropout(x=fc1, dropout_prob=0.5)\n",
    "#         fc2 = fluid.layers.fc(\n",
    "#             input=fc1,\n",
    "#             size=fc_dim,\n",
    "#             act='relu',\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Normal(scale=0.005)),\n",
    "#             bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Constant(value=0.1)))\n",
    "#         fc2 = fluid.layers.dropout(x=fc2, dropout_prob=0.5)\n",
    "#         out = fluid.layers.fc(\n",
    "#             input=fc2,\n",
    "#             size=class_dim,\n",
    "#             act='softmax',\n",
    "#             param_attr=fluid.param_attr.ParamAttr(name='fc_w',\n",
    "#                 initializer=fluid.initializer.Normal(scale=0.005)),\n",
    "#             bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Constant(value=0.1)))\n",
    "\n",
    "#         return out, conv1\n",
    "\n",
    "#     def conv_block(self, input, num_filter, groups):\n",
    "#         conv = input\n",
    "#         for i in range(groups):\n",
    "#             if i == groups - 1:\n",
    "#                 act = None\n",
    "#             else:\n",
    "#                 act = 'relu'\n",
    "#             conv = fluid.layers.conv2d(\n",
    "#                 input=conv,\n",
    "#                 num_filters=num_filter,\n",
    "#                 filter_size=3,\n",
    "#                 stride=1,\n",
    "#                 padding=1,\n",
    "#                 act=act,\n",
    "#                 param_attr=fluid.param_attr.ParamAttr(\n",
    "#                     initializer=fluid.initializer.Normal(scale=0.01)),\n",
    "#                 bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                     initializer=fluid.initializer.Constant(value=0.0)))\n",
    "#         conv = fluid.layers.batch_norm(input=conv, act='relu')\n",
    "#         return fluid.layers.pool2d(input=conv, pool_size=2, pool_type='max', pool_stride=2)\n",
    "        \n",
    "# class DenseNet(): \n",
    "#     def __init__(self, layers, dropout_prob):\n",
    "#         self.layers = layers\n",
    "#         self.dropout_prob = dropout_prob\n",
    " \n",
    "#     def bottleneck_layer(self, input, fliter_num, name):\n",
    "#         bn = fluid.layers.batch_norm(input=input, act='relu', name=name + '_bn1')\n",
    "#         conv1 = fluid.layers.conv2d(input=bn, num_filters=fliter_num * 4, filter_size=1, name=name + '_conv1')\n",
    "#         dropout = fluid.layers.dropout(x=conv1, dropout_prob=self.dropout_prob)\n",
    "\n",
    "#         bn = fluid.layers.batch_norm(input=dropout, act='relu', name=name + '_bn2')\n",
    "#         conv2 = fluid.layers.conv2d(input=bn, num_filters=fliter_num, filter_size=3, padding=1, name=name + '_conv2')\n",
    "#         dropout = fluid.layers.dropout(x=conv2, dropout_prob=self.dropout_prob)\n",
    "\n",
    "#         return dropout\n",
    "\n",
    "#     def dense_block(self, input, block_num, fliter_num, name):\n",
    "#         layers = []\n",
    "#         layers.append(input)#拼接到列表\n",
    "\n",
    "#         x = self.bottleneck_layer(input, fliter_num, name=name + '_bottle_' + str(0))\n",
    "#         layers.append(x)\n",
    "#         for i in range(block_num - 1):\n",
    "#             x = paddle.fluid.layers.concat(layers, axis=1)\n",
    "#             x = self.bottleneck_layer(x, fliter_num, name=name + '_bottle_' + str(i + 1))\n",
    "#             layers.append(x)\n",
    "\n",
    "#         return paddle.fluid.layers.concat(layers, axis=1)\n",
    "\n",
    "#     def transition_layer(self, input, fliter_num, name):\n",
    "#         bn = fluid.layers.batch_norm(input=input, act='relu', name=name + '_bn1')\n",
    "#         conv1 = fluid.layers.conv2d(input=bn, num_filters=fliter_num, filter_size=1, name=name + '_conv1') \n",
    "#         dropout = fluid.layers.dropout(x=conv1, dropout_prob=self.dropout_prob)\n",
    "        \n",
    "#         return fluid.layers.pool2d(input=dropout, pool_size=2, pool_type='avg', pool_stride=2)\n",
    " \n",
    "#     def net(self, input, class_dim=1000): \n",
    "\n",
    "#         layer_count_dict = {\n",
    "#             121: (32, [6, 12, 24, 16]),\n",
    "#             169: (32, [6, 12, 32, 32]),\n",
    "#             201: (32, [6, 12, 48, 32]),\n",
    "#             161: (48, [6, 12, 36, 24])\n",
    "#         }\n",
    "#         layer_conf = layer_count_dict[self.layers]\n",
    "\n",
    "#         conv1 = fluid.layers.conv2d(input=input, num_filters=layer_conf[0] * 2, \n",
    "#             filter_size=7, stride=2, padding=3, name='densenet_conv0')\n",
    "#         conv = fluid.layers.pool2d(input=conv1, pool_size=3, pool_padding=1, pool_type='max', pool_stride=2)\n",
    "#         for i in range(len(layer_conf[1]) - 1):\n",
    "#             conv = self.dense_block(conv, layer_conf[1][i], layer_conf[0], 'dense_' + str(i))\n",
    "#             conv = self.transition_layer(conv, layer_conf[0], name='trans_' + str(i))\n",
    "\n",
    "#         conv = self.dense_block(conv, layer_conf[1][-1], layer_conf[0], 'dense_' + str(len(layer_conf[1])))\n",
    "#         conv = fluid.layers.pool2d(input=conv, global_pooling=True, pool_type='avg')\n",
    "#         out = fluid.layers.fc(conv, class_dim, act='softmax',param_attr=fluid.ParamAttr(name='fc_w'))\n",
    "#         # last fc layer is \"out\" \n",
    "#         return out, conv1\n",
    "\n",
    "# class CNN(): \n",
    "#     def __init__(self):\n",
    "#         pass \n",
    "    \n",
    "#     def net(self, input, class_dim=1000):\n",
    "        \n",
    "#         type_size = class_dim\n",
    "#         # 第一个卷积--池化层\n",
    "#         conv_pool_1 = fluid.nets.simple_img_conv_pool(input=input,# 输入图像\n",
    "#                                                            filter_size=3,# 滤波器的大小\n",
    "#                                                            num_filters=32,# filter 的数量。它与输出的通道相同\n",
    "#                                                            pool_size=2,# 池化层大小2*2\n",
    "#                                                            pool_stride=2,# 池化层步长\n",
    "#                                                            act='relu') # 激活类型\n",
    "        \n",
    "#         # Dropout主要作用是减少过拟合，随机让某些权重不更新  \n",
    "#         # Dropout是一种正则化技术，通过在训练过程中阻止神经元节点间的联合适应性来减少过拟合。\n",
    "#         # 根据给定的丢弃概率dropout随机将一些神经元输出设置为0，其他的仍保持不变。\n",
    "#         drop_1 = fluid.layers.dropout(x=conv_pool_1, dropout_prob=0.5)\n",
    "        \n",
    "#         # 第二个卷积--池化层\n",
    "#         conv_pool_2 = fluid.nets.simple_img_conv_pool(input=drop_1,\n",
    "#                                                            filter_size=3,\n",
    "#                                                            num_filters=64,\n",
    "#                                                            pool_size=2,\n",
    "#                                                            pool_stride=2,\n",
    "#                                                            act='relu')\n",
    "#         # 减少过拟合，随机让某些权重不更新                                                   \n",
    "#         drop = fluid.layers.dropout(x=conv_pool_2, dropout_prob=0.5)\n",
    "        \n",
    "#         # 第三个卷积--池化层\n",
    "#         conv_pool_3 = fluid.nets.simple_img_conv_pool(input=drop,\n",
    "#                                                            filter_size=3,\n",
    "#                                                            num_filters=64,\n",
    "#                                                            pool_size=2,\n",
    "#                                                            pool_stride=2,\n",
    "#                                                            act='relu')\n",
    "    \n",
    "#         drop = fluid.layers.dropout(x=conv_pool_3, dropout_prob=0.5)\n",
    "       \n",
    "#        # 第四个卷积--池化层\n",
    "#         conv_pool_4 = fluid.nets.simple_img_conv_pool(input=drop,\n",
    "#                                                            filter_size=3,\n",
    "#                                                            num_filters=64,\n",
    "#                                                            pool_size=2,\n",
    "#                                                            pool_stride=2,\n",
    "#                                                            act='relu')\n",
    "#         # 减少过拟合，随机让某些权重不更新                                                   \n",
    "#         drop = fluid.layers.dropout(x=conv_pool_4, dropout_prob=0.5)\n",
    "        \n",
    "#         # 第五个卷积--池化层\n",
    "#         conv_pool_5 = fluid.nets.simple_img_conv_pool(input=drop,\n",
    "#                                                            filter_size=3,\n",
    "#                                                            num_filters=64,\n",
    "#                                                            pool_size=2,\n",
    "#                                                            pool_stride=2,\n",
    "#                                                            act='relu')\n",
    "#         # 减少过拟合，随机让某些权重不更新                                                   \n",
    "#         drop = fluid.layers.dropout(x=conv_pool_5, dropout_prob=0.5)\n",
    "#         # 全连接层\n",
    "#         fc = fluid.layers.fc(input=drop, size=512, act='relu')\n",
    "#         # 减少过拟合，随机让某些权重不更新                                                   \n",
    "#         drop =  fluid.layers.dropout(x=fc, dropout_prob=0.5)                                                   \n",
    "#         # 输出层 以softmax为激活函数的全连接输出层，输出层的大小为图像类别type_size个数\n",
    "#         predict = fluid.layers.fc(input=drop,size=type_size,act='softmax',param_attr = \"fc_w\")\n",
    "    \n",
    "        \n",
    "#         return predict, conv_pool_1\n",
    "        \n",
    "# class AlexNet():\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "#     def name(self):\n",
    "#         \"\"\"\n",
    "#         返回网络名字\n",
    "#         :return:\n",
    "#         \"\"\"\n",
    "#         return 'AlexNet'\n",
    "\n",
    "#     def net(self, input, class_dim=1000):\n",
    "#         stdv = 1.0 / math.sqrt(input.shape[1] * 11 * 11)\n",
    "#         layer_name = [\n",
    "#             \"conv1\", \"conv2\", \"conv3\", \"conv4\", \"conv5\", \"fc6\", \"fc7\", \"fc8\"\n",
    "#         ]\n",
    "#         conv1 = fluid.layers.conv2d(\n",
    "#             input=input,\n",
    "#             num_filters=64,\n",
    "#             filter_size=11,\n",
    "#             stride=4,\n",
    "#             padding=2,\n",
    "#             groups=1,\n",
    "#             act='relu',\n",
    "#             bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[0] + \"_offset\"),\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[0] + \"_weights\"))\n",
    "#         pool1 = fluid.layers.pool2d(\n",
    "#             input=conv1,\n",
    "#             pool_size=3,\n",
    "#             pool_stride=2,\n",
    "#             pool_padding=0,\n",
    "#             pool_type='max')\n",
    "\n",
    "#         stdv = 1.0 / math.sqrt(pool1.shape[1] * 5 * 5)\n",
    "#         conv2 = fluid.layers.conv2d(\n",
    "#             input=pool1,\n",
    "#             num_filters=192,\n",
    "#             filter_size=5,\n",
    "#             stride=1,\n",
    "#             padding=2,\n",
    "#             groups=1,\n",
    "#             act='relu',\n",
    "#             bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[1] + \"_offset\"),\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[1] + \"_weights\"))\n",
    "#         pool2 = fluid.layers.pool2d(\n",
    "#             input=conv2,\n",
    "#             pool_size=3,\n",
    "#             pool_stride=2,\n",
    "#             pool_padding=0,\n",
    "#             pool_type='max')\n",
    "\n",
    "#         stdv = 1.0 / math.sqrt(pool2.shape[1] * 3 * 3)\n",
    "#         conv3 = fluid.layers.conv2d(\n",
    "#             input=pool2,\n",
    "#             num_filters=384,\n",
    "#             filter_size=3,\n",
    "#             stride=1,\n",
    "#             padding=1,\n",
    "#             groups=1,\n",
    "#             act='relu',\n",
    "#             bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[2] + \"_offset\"),\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[2] + \"_weights\"))\n",
    "\n",
    "#         stdv = 1.0 / math.sqrt(conv3.shape[1] * 3 * 3)\n",
    "#         conv4 = fluid.layers.conv2d(\n",
    "#             input=conv3,\n",
    "#             num_filters=256,\n",
    "#             filter_size=3,\n",
    "#             stride=1,\n",
    "#             padding=1,\n",
    "#             groups=1,\n",
    "#             act='relu',\n",
    "#             bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[3] + \"_offset\"),\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[3] + \"_weights\"))\n",
    "\n",
    "#         stdv = 1.0 / math.sqrt(conv4.shape[1] * 3 * 3)\n",
    "#         conv5 = fluid.layers.conv2d(\n",
    "#             input=conv4,\n",
    "#             num_filters=256,\n",
    "#             filter_size=3,\n",
    "#             stride=1,\n",
    "#             padding=1,\n",
    "#             groups=1,\n",
    "#             act='relu',\n",
    "#             bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[4] + \"_offset\"),\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[4] + \"_weights\"))\n",
    "#         pool5 = fluid.layers.pool2d(\n",
    "#             input=conv5,\n",
    "#             pool_size=3,\n",
    "#             pool_stride=2,\n",
    "#             pool_padding=0,\n",
    "#             pool_type='max')\n",
    "\n",
    "#         drop6 = fluid.layers.dropout(x=pool5, dropout_prob=0.5)\n",
    "#         stdv = 1.0 / math.sqrt(drop6.shape[1] * drop6.shape[2] *\n",
    "#                                drop6.shape[3] * 1.0)\n",
    "\n",
    "#         fc6 = fluid.layers.fc(\n",
    "#             input=drop6,\n",
    "#             size=4096,\n",
    "#             act='relu',\n",
    "#             bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[5] + \"_offset\"),\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[5] + \"_weights\"))\n",
    "\n",
    "#         drop7 = fluid.layers.dropout(x=fc6, dropout_prob=0.5)\n",
    "#         stdv = 1.0 / math.sqrt(drop7.shape[1] * 1.0)\n",
    "\n",
    "#         fc7 = fluid.layers.fc(\n",
    "#             input=drop7,\n",
    "#             size=4096,\n",
    "#             act='relu',\n",
    "#             bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[6] + \"_offset\"),\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[6] + \"_weights\"))\n",
    "\n",
    "#         stdv = 1.0 / math.sqrt(fc7.shape[1] * 1.0)\n",
    "#         out = fluid.layers.fc(\n",
    "#             input=fc7,\n",
    "#             size=class_dim,\n",
    "#             act='softmax',\n",
    "#             bias_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=layer_name[7] + \"_offset\"),\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name ='fc_w'))\n",
    "#         return out, conv1\n",
    "        \n",
    "# class DistResNet():\n",
    "#     def __init__(self, layers=50, is_train=True):\n",
    "#         self.params = train_parameters\n",
    "#         self.layers = layers\n",
    "#         self.is_train = is_train\n",
    "#         self.weight_decay = 1e-4\n",
    "\n",
    "#     def net(self, input, class_dim=1000):\n",
    "#         layers = self.layers\n",
    "#         supported_layers = [50, 101, 152]\n",
    "#         assert layers in supported_layers, \\\n",
    "#             \"supported layers are {} but input layer is {}\".format(supported_layers, layers)\n",
    "\n",
    "#         if layers == 50:\n",
    "#             depth = [3, 4, 6, 3]\n",
    "#         elif layers == 101:\n",
    "#             depth = [3, 4, 23, 3]\n",
    "#         elif layers == 152:\n",
    "#             depth = [3, 8, 36, 3]\n",
    "#         num_filters = [64, 128, 256, 512]\n",
    "\n",
    "#         conv = self.conv_bn_layer(\n",
    "#             input=input, num_filters=64, filter_size=7, stride=2, act='relu')\n",
    "#         conv1 = conv\n",
    "#         conv = fluid.layers.pool2d(\n",
    "#             input=conv,\n",
    "#             pool_size=3,\n",
    "#             pool_stride=2,\n",
    "#             pool_padding=1,\n",
    "#             pool_type='max')\n",
    "\n",
    "#         for block in range(len(depth)):\n",
    "#             for i in range(depth[block]):\n",
    "#                 conv = self.bottleneck_block(\n",
    "#                     input=conv,\n",
    "#                     num_filters=num_filters[block],\n",
    "#                     stride=2 if i == 0 and block != 0 else 1)\n",
    "\n",
    "#         pool = fluid.layers.pool2d(\n",
    "#             input=conv, pool_size=7, pool_type='avg', global_pooling=True)\n",
    "#         stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)\n",
    "#         out = fluid.layers.fc(input=pool,\n",
    "#                               size=class_dim,\n",
    "#                               act=\"softmax\",\n",
    "#                               param_attr=fluid.param_attr.ParamAttr(\n",
    "#                                   name = 'fc_w',\n",
    "#                                   initializer=fluid.initializer.Uniform(-stdv,\n",
    "#                                                                         stdv),\n",
    "#                                   regularizer=fluid.regularizer.L2Decay(self.weight_decay)),\n",
    "#                               bias_attr=fluid.ParamAttr(\n",
    "#                                   regularizer=fluid.regularizer.L2Decay(self.weight_decay))\n",
    "#                               )\n",
    "#         return out, conv1\n",
    "\n",
    "#     def conv_bn_layer(self,\n",
    "#                       input,\n",
    "#                       num_filters,\n",
    "#                       filter_size,\n",
    "#                       stride=1,\n",
    "#                       groups=1,\n",
    "#                       act=None,\n",
    "#                       bn_init_value=1.0):\n",
    "#         conv = fluid.layers.conv2d(\n",
    "#             input=input,\n",
    "#             num_filters=num_filters,\n",
    "#             filter_size=filter_size,\n",
    "#             stride=stride,\n",
    "#             padding=(filter_size - 1) // 2,\n",
    "#             groups=groups,\n",
    "#             act=None,\n",
    "#             bias_attr=False,\n",
    "#             param_attr=fluid.ParamAttr(regularizer=fluid.regularizer.L2Decay(self.weight_decay)))\n",
    "#         return fluid.layers.batch_norm(\n",
    "#                 input=conv, act=act, is_test=not self.is_train,\n",
    "#                 param_attr=fluid.ParamAttr(\n",
    "#                     initializer=fluid.initializer.Constant(bn_init_value),\n",
    "#                     regularizer=None))\n",
    "\n",
    "#     def shortcut(self, input, ch_out, stride):\n",
    "#         ch_in = input.shape[1]\n",
    "#         if ch_in != ch_out or stride != 1:\n",
    "#             return self.conv_bn_layer(input, ch_out, 1, stride)\n",
    "#         else:\n",
    "#             return input\n",
    "\n",
    "#     def bottleneck_block(self, input, num_filters, stride):\n",
    "#         conv0 = self.conv_bn_layer(\n",
    "#             input=input, num_filters=num_filters, filter_size=1, act='relu')\n",
    "#         conv1 = self.conv_bn_layer(\n",
    "#             input=conv0,\n",
    "#             num_filters=num_filters,\n",
    "#             filter_size=3,\n",
    "#             stride=stride,\n",
    "#             act='relu')\n",
    "#         # NOTE: default bias is 0.0 already\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             input=conv1, num_filters=num_filters * 4, filter_size=1, act=None, bn_init_value=0.0)\n",
    "\n",
    "#         short = self.shortcut(input, num_filters * 4, stride)\n",
    "\n",
    "#         return fluid.layers.elementwise_add(x=short, y=conv2, act='relu')\n",
    "        \n",
    "# class InceptionV4():\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "    \n",
    "#     def name(self):\n",
    "#         \"\"\"\n",
    "#         返回网络名字\n",
    "#         :return:\n",
    "#         \"\"\"\n",
    "#         return 'InceptionV4'\n",
    "\n",
    "#     def net(self, input, class_dim=1000):\n",
    "#         x = self.inception_stem(input)\n",
    "#         conv1 = x\n",
    "#         for i in range(4):\n",
    "#             x = self.inceptionA(x, name=str(i + 1))\n",
    "#         x = self.reductionA(x)\n",
    "\n",
    "#         for i in range(7):\n",
    "#             x = self.inceptionB(x, name=str(i + 1))\n",
    "#         x = self.reductionB(x)\n",
    "\n",
    "#         for i in range(3):\n",
    "#             x = self.inceptionC(x, name=str(i + 1))\n",
    "\n",
    "#         pool = fluid.layers.pool2d(\n",
    "#             input=x, pool_size=8, pool_type='avg', global_pooling=True)\n",
    "\n",
    "#         drop = fluid.layers.dropout(x=pool, dropout_prob=0.2)\n",
    "\n",
    "#         stdv = 1.0 / math.sqrt(drop.shape[1] * 1.0)\n",
    "#         out = fluid.layers.fc(\n",
    "#             input=drop,\n",
    "#             size=class_dim,\n",
    "#             act='softmax',\n",
    "#             param_attr=ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name = 'fc_w'),\n",
    "#             bias_attr=ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=\"final_fc_offset\"))\n",
    "#         return out, x\n",
    "\n",
    "#     def conv_bn_layer(self,\n",
    "#                       data,\n",
    "#                       num_filters,\n",
    "#                       filter_size,\n",
    "#                       stride=1,\n",
    "#                       padding=0,\n",
    "#                       groups=1,\n",
    "#                       act='relu',\n",
    "#                       name=None):\n",
    "#         conv = fluid.layers.conv2d(\n",
    "#             input=data,\n",
    "#             num_filters=num_filters,\n",
    "#             filter_size=filter_size,\n",
    "#             stride=stride,\n",
    "#             padding=padding,\n",
    "#             groups=groups,\n",
    "#             act=None,\n",
    "#             param_attr=ParamAttr(name=name + \"_weights\"),\n",
    "#             bias_attr=False,\n",
    "#             name=name)\n",
    "#         bn_name = name + \"_bn\"\n",
    "#         return fluid.layers.batch_norm(\n",
    "#             input=conv,\n",
    "#             act=act,\n",
    "#             name=bn_name,\n",
    "#             param_attr=ParamAttr(name=bn_name + \"_scale\"),\n",
    "#             bias_attr=ParamAttr(name=bn_name + \"_offset\"),\n",
    "#             moving_mean_name=bn_name + '_mean',\n",
    "#             moving_variance_name=bn_name + '_variance')\n",
    "\n",
    "#     def inception_stem(self, data, name=None):\n",
    "#         conv = self.conv_bn_layer(\n",
    "#             data, 32, 3, stride=2, act='relu', name=\"conv1_3x3_s2\")\n",
    "#         conv = self.conv_bn_layer(conv, 32, 3, act='relu', name=\"conv2_3x3_s1\")\n",
    "#         conv = self.conv_bn_layer(\n",
    "#             conv, 64, 3, padding=1, act='relu', name=\"conv3_3x3_s1\")\n",
    "\n",
    "#         pool1 = fluid.layers.pool2d(\n",
    "#             input=conv, pool_size=3, pool_stride=2, pool_type='max')\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             conv, 96, 3, stride=2, act='relu', name=\"inception_stem1_3x3_s2\")\n",
    "#         concat = fluid.layers.concat([pool1, conv2], axis=1)\n",
    "\n",
    "#         conv1 = self.conv_bn_layer(\n",
    "#             concat, 64, 1, act='relu', name=\"inception_stem2_3x3_reduce\")\n",
    "#         conv1 = self.conv_bn_layer(\n",
    "#             conv1, 96, 3, act='relu', name=\"inception_stem2_3x3\")\n",
    "\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             concat, 64, 1, act='relu', name=\"inception_stem2_1x7_reduce\")\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             conv2,\n",
    "#             64, (7, 1),\n",
    "#             padding=(3, 0),\n",
    "#             act='relu',\n",
    "#             name=\"inception_stem2_1x7\")\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             conv2,\n",
    "#             64, (1, 7),\n",
    "#             padding=(0, 3),\n",
    "#             act='relu',\n",
    "#             name=\"inception_stem2_7x1\")\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             conv2, 96, 3, act='relu', name=\"inception_stem2_3x3_2\")\n",
    "\n",
    "#         concat = fluid.layers.concat([conv1, conv2], axis=1)\n",
    "\n",
    "#         conv1 = self.conv_bn_layer(\n",
    "#             concat, 192, 3, stride=2, act='relu', name=\"inception_stem3_3x3_s2\")\n",
    "#         pool1 = fluid.layers.pool2d(\n",
    "#             input=concat, pool_size=3, pool_stride=2, pool_type='max')\n",
    "\n",
    "#         concat = fluid.layers.concat([conv1, pool1], axis=1)\n",
    "\n",
    "#         return concat\n",
    "\n",
    "#     def inceptionA(self, data, name=None):\n",
    "#         pool1 = fluid.layers.pool2d(\n",
    "#             input=data, pool_size=3, pool_padding=1, pool_type='avg')\n",
    "#         conv1 = self.conv_bn_layer(\n",
    "#             pool1, 96, 1, act='relu', name=\"inception_a\" + name + \"_1x1\")\n",
    "\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             data, 96, 1, act='relu', name=\"inception_a\" + name + \"_1x1_2\")\n",
    "\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             data, 64, 1, act='relu', name=\"inception_a\" + name + \"_3x3_reduce\")\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             conv3,\n",
    "#             96,\n",
    "#             3,\n",
    "#             padding=1,\n",
    "#             act='relu',\n",
    "#             name=\"inception_a\" + name + \"_3x3\")\n",
    "\n",
    "#         conv4 = self.conv_bn_layer(\n",
    "#             data,\n",
    "#             64,\n",
    "#             1,\n",
    "#             act='relu',\n",
    "#             name=\"inception_a\" + name + \"_3x3_2_reduce\")\n",
    "#         conv4 = self.conv_bn_layer(\n",
    "#             conv4,\n",
    "#             96,\n",
    "#             3,\n",
    "#             padding=1,\n",
    "#             act='relu',\n",
    "#             name=\"inception_a\" + name + \"_3x3_2\")\n",
    "#         conv4 = self.conv_bn_layer(\n",
    "#             conv4,\n",
    "#             96,\n",
    "#             3,\n",
    "#             padding=1,\n",
    "#             act='relu',\n",
    "#             name=\"inception_a\" + name + \"_3x3_3\")\n",
    "\n",
    "#         concat = fluid.layers.concat([conv1, conv2, conv3, conv4], axis=1)\n",
    "\n",
    "#         return concat\n",
    "\n",
    "#     def reductionA(self, data, name=None):\n",
    "#         pool1 = fluid.layers.pool2d(\n",
    "#             input=data, pool_size=3, pool_stride=2, pool_type='max')\n",
    "\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             data, 384, 3, stride=2, act='relu', name=\"reduction_a_3x3\")\n",
    "\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             data, 192, 1, act='relu', name=\"reduction_a_3x3_2_reduce\")\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             conv3, 224, 3, padding=1, act='relu', name=\"reduction_a_3x3_2\")\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             conv3, 256, 3, stride=2, act='relu', name=\"reduction_a_3x3_3\")\n",
    "\n",
    "#         concat = fluid.layers.concat([pool1, conv2, conv3], axis=1)\n",
    "\n",
    "#         return concat\n",
    "\n",
    "#     def inceptionB(self, data, name=None):\n",
    "#         pool1 = fluid.layers.pool2d(\n",
    "#             input=data, pool_size=3, pool_padding=1, pool_type='avg')\n",
    "#         conv1 = self.conv_bn_layer(\n",
    "#             pool1, 128, 1, act='relu', name=\"inception_b\" + name + \"_1x1\")\n",
    "\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             data, 384, 1, act='relu', name=\"inception_b\" + name + \"_1x1_2\")\n",
    "\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             data, 192, 1, act='relu', name=\"inception_b\" + name + \"_1x7_reduce\")\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             conv3,\n",
    "#             224, (1, 7),\n",
    "#             padding=(0, 3),\n",
    "#             act='relu',\n",
    "#             name=\"inception_b\" + name + \"_1x7\")\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             conv3,\n",
    "#             256, (7, 1),\n",
    "#             padding=(3, 0),\n",
    "#             act='relu',\n",
    "#             name=\"inception_b\" + name + \"_7x1\")\n",
    "\n",
    "#         conv4 = self.conv_bn_layer(\n",
    "#             data,\n",
    "#             192,\n",
    "#             1,\n",
    "#             act='relu',\n",
    "#             name=\"inception_b\" + name + \"_7x1_2_reduce\")\n",
    "#         conv4 = self.conv_bn_layer(\n",
    "#             conv4,\n",
    "#             192, (1, 7),\n",
    "#             padding=(0, 3),\n",
    "#             act='relu',\n",
    "#             name=\"inception_b\" + name + \"_1x7_2\")\n",
    "#         conv4 = self.conv_bn_layer(\n",
    "#             conv4,\n",
    "#             224, (7, 1),\n",
    "#             padding=(3, 0),\n",
    "#             act='relu',\n",
    "#             name=\"inception_b\" + name + \"_7x1_2\")\n",
    "#         conv4 = self.conv_bn_layer(\n",
    "#             conv4,\n",
    "#             224, (1, 7),\n",
    "#             padding=(0, 3),\n",
    "#             act='relu',\n",
    "#             name=\"inception_b\" + name + \"_1x7_3\")\n",
    "#         conv4 = self.conv_bn_layer(\n",
    "#             conv4,\n",
    "#             256, (7, 1),\n",
    "#             padding=(3, 0),\n",
    "#             act='relu',\n",
    "#             name=\"inception_b\" + name + \"_7x1_3\")\n",
    "\n",
    "#         concat = fluid.layers.concat([conv1, conv2, conv3, conv4], axis=1)\n",
    "\n",
    "#         return concat\n",
    "\n",
    "#     def reductionB(self, data, name=None):\n",
    "#         pool1 = fluid.layers.pool2d(\n",
    "#             input=data, pool_size=3, pool_stride=2, pool_type='max')\n",
    "\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             data, 192, 1, act='relu', name=\"reduction_b_3x3_reduce\")\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             conv2, 192, 3, stride=2, act='relu', name=\"reduction_b_3x3\")\n",
    "\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             data, 256, 1, act='relu', name=\"reduction_b_1x7_reduce\")\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             conv3,\n",
    "#             256, (1, 7),\n",
    "#             padding=(0, 3),\n",
    "#             act='relu',\n",
    "#             name=\"reduction_b_1x7\")\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             conv3,\n",
    "#             320, (7, 1),\n",
    "#             padding=(3, 0),\n",
    "#             act='relu',\n",
    "#             name=\"reduction_b_7x1\")\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             conv3, 320, 3, stride=2, act='relu', name=\"reduction_b_3x3_2\")\n",
    "\n",
    "#         concat = fluid.layers.concat([pool1, conv2, conv3], axis=1)\n",
    "\n",
    "#         return concat\n",
    "\n",
    "#     def inceptionC(self, data, name=None):\n",
    "#         pool1 = fluid.layers.pool2d(\n",
    "#             input=data, pool_size=3, pool_padding=1, pool_type='avg')\n",
    "#         conv1 = self.conv_bn_layer(\n",
    "#             pool1, 256, 1, act='relu', name=\"inception_c\" + name + \"_1x1\")\n",
    "\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             data, 256, 1, act='relu', name=\"inception_c\" + name + \"_1x1_2\")\n",
    "\n",
    "#         conv3 = self.conv_bn_layer(\n",
    "#             data, 384, 1, act='relu', name=\"inception_c\" + name + \"_1x1_3\")\n",
    "#         conv3_1 = self.conv_bn_layer(\n",
    "#             conv3,\n",
    "#             256, (1, 3),\n",
    "#             padding=(0, 1),\n",
    "#             act='relu',\n",
    "#             name=\"inception_c\" + name + \"_1x3\")\n",
    "#         conv3_2 = self.conv_bn_layer(\n",
    "#             conv3,\n",
    "#             256, (3, 1),\n",
    "#             padding=(1, 0),\n",
    "#             act='relu',\n",
    "#             name=\"inception_c\" + name + \"_3x1\")\n",
    "\n",
    "#         conv4 = self.conv_bn_layer(\n",
    "#             data, 384, 1, act='relu', name=\"inception_c\" + name + \"_1x1_4\")\n",
    "#         conv4 = self.conv_bn_layer(\n",
    "#             conv4,\n",
    "#             448, (1, 3),\n",
    "#             padding=(0, 1),\n",
    "#             act='relu',\n",
    "#             name=\"inception_c\" + name + \"_1x3_2\")\n",
    "#         conv4 = self.conv_bn_layer(\n",
    "#             conv4,\n",
    "#             512, (3, 1),\n",
    "#             padding=(1, 0),\n",
    "#             act='relu',\n",
    "#             name=\"inception_c\" + name + \"_3x1_2\")\n",
    "#         conv4_1 = self.conv_bn_layer(\n",
    "#             conv4,\n",
    "#             256, (1, 3),\n",
    "#             padding=(0, 1),\n",
    "#             act='relu',\n",
    "#             name=\"inception_c\" + name + \"_1x3_3\")\n",
    "#         conv4_2 = self.conv_bn_layer(\n",
    "#             conv4,\n",
    "#             256, (3, 1),\n",
    "#             padding=(1, 0),\n",
    "#             act='relu',\n",
    "#             name=\"inception_c\" + name + \"_3x1_3\")\n",
    "\n",
    "#         concat = fluid.layers.concat(\n",
    "#             [conv1, conv2, conv3_1, conv3_2, conv4_1, conv4_2], axis=1)\n",
    "\n",
    "#         return concat\n",
    "        \n",
    "# class SE_ResNeXt():\n",
    "#     def __init__(self, layers=50):\n",
    "#         self.params = train_parameters\n",
    "#         self.layers = layers\n",
    "\n",
    "#     def net(self, input, class_dim=1000):\n",
    "#         layers = self.layers\n",
    "#         supported_layers = [50, 101, 152]\n",
    "#         assert layers in supported_layers, \\\n",
    "#             \"supported layers are {} but input layer is {}\".format(supported_layers, layers)\n",
    "#         if layers == 50:\n",
    "#             cardinality = 32\n",
    "#             reduction_ratio = 16\n",
    "#             depth = [3, 4, 6, 3]\n",
    "#             num_filters = [128, 256, 512, 1024]\n",
    "\n",
    "#             conv = self.conv_bn_layer(\n",
    "#                 input=input,\n",
    "#                 num_filters=64,\n",
    "#                 filter_size=7,\n",
    "#                 stride=2,\n",
    "#                 act='relu',\n",
    "#                 name='conv1', )\n",
    "#             conv1 = conv\n",
    "#             conv = fluid.layers.pool2d(\n",
    "#                 input=conv,\n",
    "#                 pool_size=3,\n",
    "#                 pool_stride=2,\n",
    "#                 pool_padding=1,\n",
    "#                 pool_type='max')\n",
    "#         elif layers == 101:\n",
    "#             cardinality = 32\n",
    "#             reduction_ratio = 16\n",
    "#             depth = [3, 4, 23, 3]\n",
    "#             num_filters = [128, 256, 512, 1024]\n",
    "\n",
    "#             conv = self.conv_bn_layer(\n",
    "#                 input=input,\n",
    "#                 num_filters=64,\n",
    "#                 filter_size=7,\n",
    "#                 stride=2,\n",
    "#                 act='relu',\n",
    "#                 name=\"conv1\", )\n",
    "#             conv = fluid.layers.pool2d(\n",
    "#                 input=conv,\n",
    "#                 pool_size=3,\n",
    "#                 pool_stride=2,\n",
    "#                 pool_padding=1,\n",
    "#                 pool_type='max')\n",
    "#         elif layers == 152:\n",
    "#             cardinality = 64\n",
    "#             reduction_ratio = 16\n",
    "#             depth = [3, 8, 36, 3]\n",
    "#             num_filters = [128, 256, 512, 1024]\n",
    "\n",
    "#             conv = self.conv_bn_layer(\n",
    "#                 input=input,\n",
    "#                 num_filters=64,\n",
    "#                 filter_size=3,\n",
    "#                 stride=2,\n",
    "#                 act='relu',\n",
    "#                 name='conv1')\n",
    "#             conv = self.conv_bn_layer(\n",
    "#                 input=conv,\n",
    "#                 num_filters=64,\n",
    "#                 filter_size=3,\n",
    "#                 stride=1,\n",
    "#                 act='relu',\n",
    "#                 name='conv2')\n",
    "#             conv = self.conv_bn_layer(\n",
    "#                 input=conv,\n",
    "#                 num_filters=128,\n",
    "#                 filter_size=3,\n",
    "#                 stride=1,\n",
    "#                 act='relu',\n",
    "#                 name='conv3')\n",
    "#             conv = fluid.layers.pool2d(\n",
    "#                 input=conv, pool_size=3, pool_stride=2, pool_padding=1, \\\n",
    "#                 pool_type='max')\n",
    "#         n = 1 if layers == 50 or layers == 101 else 3\n",
    "#         for block in range(len(depth)):\n",
    "#             n += 1\n",
    "#             for i in range(depth[block]):\n",
    "#                 conv = self.bottleneck_block(\n",
    "#                     input=conv,\n",
    "#                     num_filters=num_filters[block],\n",
    "#                     stride=2 if i == 0 and block != 0 else 1,\n",
    "#                     cardinality=cardinality,\n",
    "#                     reduction_ratio=reduction_ratio,\n",
    "#                     name=str(n) + '_' + str(i + 1))\n",
    "\n",
    "#         pool = fluid.layers.pool2d(\n",
    "#             input=conv, pool_size=7, pool_type='avg', global_pooling=True)\n",
    "#         drop = fluid.layers.dropout(\n",
    "#             x=pool, dropout_prob=0.5, seed=self.params['dropout_seed'])\n",
    "#         stdv = 1.0 / math.sqrt(drop.shape[1] * 1.0)\n",
    "#         out = fluid.layers.fc(\n",
    "#             input=drop,\n",
    "#             size=class_dim,\n",
    "#             act=\"softmax\",\n",
    "#             param_attr=ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name='fc_w'),\n",
    "#             bias_attr=ParamAttr(name='fc6_offset'))\n",
    "#         return out, conv1\n",
    "\n",
    "#     def shortcut(self, input, ch_out, stride, name):\n",
    "#         ch_in = input.shape[1]\n",
    "#         if ch_in != ch_out or stride != 1:\n",
    "#             filter_size = 1\n",
    "#             return self.conv_bn_layer(\n",
    "#                 input, ch_out, filter_size, stride, name='conv' + name + '_prj')\n",
    "#         else:\n",
    "#             return input\n",
    "\n",
    "#     def bottleneck_block(self,\n",
    "#                          input,\n",
    "#                          num_filters,\n",
    "#                          stride,\n",
    "#                          cardinality,\n",
    "#                          reduction_ratio,\n",
    "#                          name=None):\n",
    "#         conv0 = self.conv_bn_layer(\n",
    "#             input=input,\n",
    "#             num_filters=num_filters,\n",
    "#             filter_size=1,\n",
    "#             act='relu',\n",
    "#             name='conv' + name + '_x1')\n",
    "#         conv1 = self.conv_bn_layer(\n",
    "#             input=conv0,\n",
    "#             num_filters=num_filters,\n",
    "#             filter_size=3,\n",
    "#             stride=stride,\n",
    "#             groups=cardinality,\n",
    "#             act='relu',\n",
    "#             name='conv' + name + '_x2')\n",
    "#         conv2 = self.conv_bn_layer(\n",
    "#             input=conv1,\n",
    "#             num_filters=num_filters * 2,\n",
    "#             filter_size=1,\n",
    "#             act=None,\n",
    "#             name='conv' + name + '_x3')\n",
    "#         scale = self.squeeze_excitation(\n",
    "#             input=conv2,\n",
    "#             num_channels=num_filters * 2,\n",
    "#             reduction_ratio=reduction_ratio,\n",
    "#             name='fc' + name)\n",
    "\n",
    "#         short = self.shortcut(input, num_filters * 2, stride, name=name)\n",
    "\n",
    "#         return fluid.layers.elementwise_add(x=short, y=scale, act='relu')\n",
    "\n",
    "#     def conv_bn_layer(self,\n",
    "#                       input,\n",
    "#                       num_filters,\n",
    "#                       filter_size,\n",
    "#                       stride=1,\n",
    "#                       groups=1,\n",
    "#                       act=None,\n",
    "#                       name=None):\n",
    "#         conv = fluid.layers.conv2d(\n",
    "#             input=input,\n",
    "#             num_filters=num_filters,\n",
    "#             filter_size=filter_size,\n",
    "#             stride=stride,\n",
    "#             padding=(filter_size - 1) // 2,\n",
    "#             groups=groups,\n",
    "#             act=None,\n",
    "#             bias_attr=False,\n",
    "#             param_attr=ParamAttr(name=name + '_weights'), )\n",
    "#         bn_name = name + \"_bn\"\n",
    "#         return fluid.layers.batch_norm(\n",
    "#             input=conv,\n",
    "#             act=act,\n",
    "#             param_attr=ParamAttr(name=bn_name + '_scale'),\n",
    "#             bias_attr=ParamAttr(bn_name + '_offset'),\n",
    "#             moving_mean_name=bn_name + '_mean',\n",
    "#             moving_variance_name=bn_name + '_variance')\n",
    "\n",
    "#     def squeeze_excitation(self,\n",
    "#                            input,\n",
    "#                            num_channels,\n",
    "#                            reduction_ratio,\n",
    "#                            name=None):\n",
    "#         pool = fluid.layers.pool2d(\n",
    "#             input=input, pool_size=0, pool_type='avg', global_pooling=True)\n",
    "#         stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)\n",
    "#         squeeze = fluid.layers.fc(\n",
    "#             input=pool,\n",
    "#             size=num_channels // reduction_ratio,\n",
    "#             act='relu',\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=name + '_sqz_weights'),\n",
    "#             bias_attr=ParamAttr(name=name + '_sqz_offset'))\n",
    "#         stdv = 1.0 / math.sqrt(squeeze.shape[1] * 1.0)\n",
    "#         excitation = fluid.layers.fc(\n",
    "#             input=squeeze,\n",
    "#             size=num_channels,\n",
    "#             act='sigmoid',\n",
    "#             param_attr=fluid.param_attr.ParamAttr(\n",
    "#                 initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#                 name=name + '_exc_weights'),\n",
    "#             bias_attr=ParamAttr(name=name + '_exc_offset'))\n",
    "#         scale = fluid.layers.elementwise_mul(x=input, y=excitation, axis=0)\n",
    "#         return scale\n",
    "        \n",
    "# class DPN(object):\n",
    "#     def __init__(self, layers=68):\n",
    "#         self.layers = layers\n",
    "\n",
    "#     def net(self, input, class_dim=1000):\n",
    "#         # get network args\n",
    "#         args = self.get_net_args(self.layers)\n",
    "#         bws = args['bw']\n",
    "#         inc_sec = args['inc_sec']\n",
    "#         rs = args['bw']\n",
    "#         k_r = args['k_r']\n",
    "#         k_sec = args['k_sec']\n",
    "#         G = args['G']\n",
    "#         init_num_filter = args['init_num_filter']\n",
    "#         init_filter_size = args['init_filter_size']\n",
    "#         init_padding = args['init_padding']\n",
    "\n",
    "#         ## define Dual Path Network\n",
    "\n",
    "#         # conv1\n",
    "#         conv1_x_1 = fluid.layers.conv2d(\n",
    "#             input=input,\n",
    "#             num_filters=init_num_filter,\n",
    "#             filter_size=init_filter_size,\n",
    "#             stride=2,\n",
    "#             padding=init_padding,\n",
    "#             groups=1,\n",
    "#             act=None,\n",
    "#             bias_attr=False,\n",
    "#             name=\"conv1\",\n",
    "#             param_attr=ParamAttr(name=\"conv1_weights\"), )\n",
    "\n",
    "#         conv1 = conv1_x_1\n",
    "\n",
    "#         conv1_x_1 = fluid.layers.batch_norm(\n",
    "#             input=conv1_x_1,\n",
    "#             act='relu',\n",
    "#             is_test=False,\n",
    "#             name=\"conv1_bn\",\n",
    "#             param_attr=ParamAttr(name='conv1_bn_scale'),\n",
    "#             bias_attr=ParamAttr('conv1_bn_offset'),\n",
    "#             moving_mean_name='conv1_bn_mean',\n",
    "#             moving_variance_name='conv1_bn_variance', )\n",
    "\n",
    "#         convX_x_x = fluid.layers.pool2d(\n",
    "#             input=conv1_x_1,\n",
    "#             pool_size=3,\n",
    "#             pool_stride=2,\n",
    "#             pool_padding=1,\n",
    "#             pool_type='max',\n",
    "#             name=\"pool1\")\n",
    "\n",
    "#         #conv2 - conv5\n",
    "#         match_list, num = [], 0\n",
    "#         for gc in range(4):\n",
    "#             bw = bws[gc]\n",
    "#             inc = inc_sec[gc]\n",
    "#             R = (k_r * bw) // rs[gc]\n",
    "#             if gc == 0:\n",
    "#                 _type1 = 'proj'\n",
    "#                 _type2 = 'normal'\n",
    "#                 match = 1\n",
    "#             else:\n",
    "#                 _type1 = 'down'\n",
    "#                 _type2 = 'normal'\n",
    "#                 match = match + k_sec[gc - 1]\n",
    "#             match_list.append(match)\n",
    "\n",
    "#             convX_x_x = self.dual_path_factory(\n",
    "#                 convX_x_x, R, R, bw, inc, G, _type1, name=\"dpn\" + str(match))\n",
    "#             for i_ly in range(2, k_sec[gc] + 1):\n",
    "#                 num += 1\n",
    "#                 if num in match_list:\n",
    "#                     num += 1\n",
    "#                 convX_x_x = self.dual_path_factory(\n",
    "#                     convX_x_x, R, R, bw, inc, G, _type2, name=\"dpn\" + str(num))\n",
    "\n",
    "#         conv5_x_x = fluid.layers.concat(convX_x_x, axis=1)\n",
    "#         conv5_x_x = fluid.layers.batch_norm(\n",
    "#             input=conv5_x_x,\n",
    "#             act='relu',\n",
    "#             is_test=False,\n",
    "#             name=\"final_concat_bn\",\n",
    "#             param_attr=ParamAttr(name='final_concat_bn_scale'),\n",
    "#             bias_attr=ParamAttr('final_concat_bn_offset'),\n",
    "#             moving_mean_name='final_concat_bn_mean',\n",
    "#             moving_variance_name='final_concat_bn_variance', )\n",
    "#         pool5 = fluid.layers.pool2d(\n",
    "#             input=conv5_x_x,\n",
    "#             pool_size=7,\n",
    "#             pool_stride=1,\n",
    "#             pool_padding=0,\n",
    "#             pool_type='avg', )\n",
    "\n",
    "#         stdv = 0.01\n",
    "#         param_attr = fluid.param_attr.ParamAttr(\n",
    "#             initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "#             name = 'fc_w')\n",
    "#         fc6 = fluid.layers.fc(input=pool5,\n",
    "#                               size=class_dim,\n",
    "#                               act=\"softmax\",\n",
    "#                               param_attr=param_attr,\n",
    "#                               name=\"fc6\")\n",
    "\n",
    "#         return fc6, conv1\n",
    "\n",
    "#     def get_net_args(self, layers):\n",
    "#         if layers == 68:\n",
    "#             k_r = 128\n",
    "#             G = 32\n",
    "#             k_sec = [3, 4, 12, 3]\n",
    "#             inc_sec = [16, 32, 32, 64]\n",
    "#             bw = [64, 128, 256, 512]\n",
    "#             r = [64, 64, 64, 64]\n",
    "#             init_num_filter = 10\n",
    "#             init_filter_size = 3\n",
    "#             init_padding = 1\n",
    "#         elif layers == 92:\n",
    "#             k_r = 96\n",
    "#             G = 32\n",
    "#             k_sec = [3, 4, 20, 3]\n",
    "#             inc_sec = [16, 32, 24, 128]\n",
    "#             bw = [256, 512, 1024, 2048]\n",
    "#             r = [256, 256, 256, 256]\n",
    "#             init_num_filter = 64\n",
    "#             init_filter_size = 7\n",
    "#             init_padding = 3\n",
    "#         elif layers == 98:\n",
    "#             k_r = 160\n",
    "#             G = 40\n",
    "#             k_sec = [3, 6, 20, 3]\n",
    "#             inc_sec = [16, 32, 32, 128]\n",
    "#             bw = [256, 512, 1024, 2048]\n",
    "#             r = [256, 256, 256, 256]\n",
    "#             init_num_filter = 96\n",
    "#             init_filter_size = 7\n",
    "#             init_padding = 3\n",
    "#         elif layers == 107:\n",
    "#             k_r = 200\n",
    "#             G = 50\n",
    "#             k_sec = [4, 8, 20, 3]\n",
    "#             inc_sec = [20, 64, 64, 128]\n",
    "#             bw = [256, 512, 1024, 2048]\n",
    "#             r = [256, 256, 256, 256]\n",
    "#             init_num_filter = 128\n",
    "#             init_filter_size = 7\n",
    "#             init_padding = 3\n",
    "#         elif layers == 131:\n",
    "#             k_r = 160\n",
    "#             G = 40\n",
    "#             k_sec = [4, 8, 28, 3]\n",
    "#             inc_sec = [16, 32, 32, 128]\n",
    "#             bw = [256, 512, 1024, 2048]\n",
    "#             r = [256, 256, 256, 256]\n",
    "#             init_num_filter = 128\n",
    "#             init_filter_size = 7\n",
    "#             init_padding = 3\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "#         net_arg = {\n",
    "#             'k_r': k_r,\n",
    "#             'G': G,\n",
    "#             'k_sec': k_sec,\n",
    "#             'inc_sec': inc_sec,\n",
    "#             'bw': bw,\n",
    "#             'r': r\n",
    "#         }\n",
    "#         net_arg['init_num_filter'] = init_num_filter\n",
    "#         net_arg['init_filter_size'] = init_filter_size\n",
    "#         net_arg['init_padding'] = init_padding\n",
    "\n",
    "#         return net_arg\n",
    "\n",
    "#     def dual_path_factory(self,\n",
    "#                           data,\n",
    "#                           num_1x1_a,\n",
    "#                           num_3x3_b,\n",
    "#                           num_1x1_c,\n",
    "#                           inc,\n",
    "#                           G,\n",
    "#                           _type='normal',\n",
    "#                           name=None):\n",
    "#         kw = 3\n",
    "#         kh = 3\n",
    "#         pw = (kw - 1) // 2\n",
    "#         ph = (kh - 1) // 2\n",
    "\n",
    "#         # type\n",
    "#         if _type is 'proj':\n",
    "#             key_stride = 1\n",
    "#             has_proj = True\n",
    "#         if _type is 'down':\n",
    "#             key_stride = 2\n",
    "#             has_proj = True\n",
    "#         if _type is 'normal':\n",
    "#             key_stride = 1\n",
    "#             has_proj = False\n",
    "\n",
    "#         # PROJ\n",
    "#         if type(data) is list:\n",
    "#             data_in = fluid.layers.concat([data[0], data[1]], axis=1)\n",
    "#         else:\n",
    "#             data_in = data\n",
    "\n",
    "#         if has_proj:\n",
    "#             c1x1_w = self.bn_ac_conv(\n",
    "#                 data=data_in,\n",
    "#                 num_filter=(num_1x1_c + 2 * inc),\n",
    "#                 kernel=(1, 1),\n",
    "#                 pad=(0, 0),\n",
    "#                 stride=(key_stride, key_stride),\n",
    "#                 name=name + \"_match\")\n",
    "#             data_o1, data_o2 = fluid.layers.split(\n",
    "#                 c1x1_w,\n",
    "#                 num_or_sections=[num_1x1_c, 2 * inc],\n",
    "#                 dim=1,\n",
    "#                 name=name + \"_match_conv_Slice\")\n",
    "#         else:\n",
    "#             data_o1 = data[0]\n",
    "#             data_o2 = data[1]\n",
    "\n",
    "#         # MAIN\n",
    "#         c1x1_a = self.bn_ac_conv(\n",
    "#             data=data_in,\n",
    "#             num_filter=num_1x1_a,\n",
    "#             kernel=(1, 1),\n",
    "#             pad=(0, 0),\n",
    "#             name=name + \"_conv1\")\n",
    "#         c3x3_b = self.bn_ac_conv(\n",
    "#             data=c1x1_a,\n",
    "#             num_filter=num_3x3_b,\n",
    "#             kernel=(kw, kh),\n",
    "#             pad=(pw, ph),\n",
    "#             stride=(key_stride, key_stride),\n",
    "#             num_group=G,\n",
    "#             name=name + \"_conv2\")\n",
    "#         c1x1_c = self.bn_ac_conv(\n",
    "#             data=c3x3_b,\n",
    "#             num_filter=(num_1x1_c + inc),\n",
    "#             kernel=(1, 1),\n",
    "#             pad=(0, 0),\n",
    "#             name=name + \"_conv3\")\n",
    "\n",
    "#         c1x1_c1, c1x1_c2 = fluid.layers.split(\n",
    "#             c1x1_c,\n",
    "#             num_or_sections=[num_1x1_c, inc],\n",
    "#             dim=1,\n",
    "#             name=name + \"_conv3_Slice\")\n",
    "\n",
    "#         # OUTPUTS\n",
    "#         summ = fluid.layers.elementwise_add(\n",
    "#             x=data_o1, y=c1x1_c1, name=name + \"_elewise\")\n",
    "#         dense = fluid.layers.concat(\n",
    "#             [data_o2, c1x1_c2], axis=1, name=name + \"_concat\")\n",
    "\n",
    "#         return [summ, dense]\n",
    "\n",
    "#     def bn_ac_conv(self,\n",
    "#                    data,\n",
    "#                    num_filter,\n",
    "#                    kernel,\n",
    "#                    pad,\n",
    "#                    stride=(1, 1),\n",
    "#                    num_group=1,\n",
    "#                    name=None):\n",
    "#         bn_ac = fluid.layers.batch_norm(\n",
    "#             input=data,\n",
    "#             act='relu',\n",
    "#             is_test=False,\n",
    "#             name=name + '.output.1',\n",
    "#             param_attr=ParamAttr(name=name + '_bn_scale'),\n",
    "#             bias_attr=ParamAttr(name + '_bn_offset'),\n",
    "#             moving_mean_name=name + '_bn_mean',\n",
    "#             moving_variance_name=name + '_bn_variance', )\n",
    "#         bn_ac_conv = fluid.layers.conv2d(\n",
    "#             input=bn_ac,\n",
    "#             num_filters=num_filter,\n",
    "#             filter_size=kernel,\n",
    "#             stride=stride,\n",
    "#             padding=pad,\n",
    "#             groups=num_group,\n",
    "#             act=None,\n",
    "#             bias_attr=False,\n",
    "#             param_attr=ParamAttr(name=name + \"_weights\"))\n",
    "#         return bn_ac_conv\n",
    "        \n",
    "# class ShuffleNetV2():\n",
    "#     def __init__(self, scale=1.0):\n",
    "#         self.scale = scale\n",
    "\n",
    "#     def net(self, input, class_dim=1000):\n",
    "#         scale = self.scale \n",
    "#         stage_repeats = [4, 8, 4]\n",
    "        \n",
    "#         if scale == 0.5:\n",
    "#             stage_out_channels = [-1, 24,  48,  96, 192, 1024]\n",
    "#         elif scale == 1.0:\n",
    "#             stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n",
    "#         elif scale == 1.5:\n",
    "#             stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n",
    "#         elif scale == 2.0:\n",
    "#             stage_out_channels = [-1, 24, 224, 488, 976, 2048]\n",
    "#         elif scale == 8.0:\n",
    "#             stage_out_channels = [-1, 48, 896, 1952, 3904, 8192]\n",
    "#         else:\n",
    "#             raise ValueError(\n",
    "#                 \"\"\"{} groups is not supported for\n",
    "#                        1x1 Grouped Convolutions\"\"\".format(num_groups))\n",
    "\n",
    "#         #conv1\n",
    "        \n",
    "#         input_channel = stage_out_channels[1]\n",
    "#         conv1 = self.conv_bn_layer(input=input, filter_size=3, num_filters=input_channel, padding=1, stride=2,name='stage1_conv')    \n",
    "#         pool1 = fluid.layers.pool2d(input=conv1, pool_size=3, pool_stride=2, pool_padding=1, pool_type='max')\n",
    "#         conv = pool1\n",
    "#         # bottleneck sequences\n",
    "#         for idxstage in range(len(stage_repeats)):\n",
    "#             numrepeat = stage_repeats[idxstage]\n",
    "#             output_channel = stage_out_channels[idxstage+2]\n",
    "#             for i in range(numrepeat):\n",
    "#                 if i == 0:\n",
    "#                     conv = self.inverted_residual_unit(input=conv, num_filters=output_channel, stride=2, \n",
    "#                                                        benchmodel=2,name=str(idxstage+2)+'_'+str(i+1))\n",
    "#                 else:\n",
    "#                     conv = self.inverted_residual_unit(input=conv, num_filters=output_channel, stride=1, \n",
    "#                                                        benchmodel=1,name=str(idxstage+2)+'_'+str(i+1))\n",
    "                \n",
    "#         conv_last = self.conv_bn_layer(input=conv, filter_size=1, num_filters=stage_out_channels[-1], \n",
    "#                                        padding=0, stride=1, name='conv5')\n",
    "#         pool_last = fluid.layers.pool2d(input=conv_last, pool_size=7, pool_stride=1, pool_padding=0, pool_type='avg')\n",
    "\n",
    "\n",
    "#         output = fluid.layers.fc(input=pool_last,\n",
    "#                                  size=class_dim,\n",
    "#                                  act=\"softmax\",\n",
    "#                                  param_attr=ParamAttr(initializer=MSRA(),name='fc_w'),\n",
    "#                                  bias_attr=ParamAttr(name='fc6_offset'))\n",
    "#         return output, conv1\n",
    "\n",
    "    \n",
    "#     def conv_bn_layer(self,\n",
    "#                   input,\n",
    "#                   filter_size,\n",
    "#                   num_filters,\n",
    "#                   stride,\n",
    "#                   padding,\n",
    "#                   num_groups=1,\n",
    "#                   use_cudnn=True,\n",
    "#                   if_act=True,\n",
    "#                   name=None):\n",
    "# #         print(num_groups)\n",
    "#         conv = fluid.layers.conv2d(\n",
    "#             input=input,\n",
    "#             num_filters=num_filters,\n",
    "#             filter_size=filter_size,\n",
    "#             stride=stride,\n",
    "#             padding=padding,\n",
    "#             groups=num_groups,\n",
    "#             act=None,\n",
    "#             use_cudnn=use_cudnn,\n",
    "#             param_attr=ParamAttr(initializer=MSRA(),name=name+'_weights'),\n",
    "#             bias_attr=False)\n",
    "#         out = int((input.shape[2] - 1)/float(stride) + 1)\n",
    "#        # print(input.shape[1],(out, out), num_filters, (filter_size, filter_size), stride, \n",
    "#        #       (filter_size - 1) / 2, num_groups, name)\n",
    "#         bn_name = name + '_bn'\n",
    "#         if if_act:\n",
    "#             return fluid.layers.batch_norm(input=conv, act='swish',\n",
    "#                                            param_attr = ParamAttr(name=bn_name+\"_scale\"),\n",
    "#                                            bias_attr=ParamAttr(name=bn_name+\"_offset\"),\n",
    "#                                            moving_mean_name=bn_name + '_mean',\n",
    "#                                            moving_variance_name=bn_name + '_variance')\n",
    "#         else:\n",
    "#             return fluid.layers.batch_norm(input=conv,\n",
    "#                                            param_attr = ParamAttr(name=bn_name+\"_scale\"),\n",
    "#                                            bias_attr=ParamAttr(name=bn_name+\"_offset\"),\n",
    "#                                            moving_mean_name=bn_name + '_mean',\n",
    "#                                            moving_variance_name=bn_name + '_variance')\n",
    "\n",
    "      \n",
    "#     def channel_shuffle(self, x, groups):\n",
    "#         batchsize, num_channels, height, width = x.shape[0], x.shape[1], x.shape[2], x.shape[3]\n",
    "#         channels_per_group = num_channels // groups\n",
    "    \n",
    "#         # reshape\n",
    "#         x = fluid.layers.reshape(x=x, shape=[batchsize, groups, channels_per_group, height, width])\n",
    "\n",
    "#         x = fluid.layers.transpose(x=x, perm=[0,2,1,3,4])\n",
    "\n",
    "#         # flatten\n",
    "#         x = fluid.layers.reshape(x=x, shape=[batchsize, num_channels, height, width])\n",
    "\n",
    "#         return x\n",
    "\n",
    "    \n",
    "#     def inverted_residual_unit(self, input, num_filters, stride, benchmodel, name=None):\n",
    "#         assert stride in [1, 2], \\\n",
    "#             \"supported stride are {} but your stride is {}\".format([1,2], stride)\n",
    "            \n",
    "#         oup_inc = num_filters//2\n",
    "#         inp = input.shape[1]\n",
    "        \n",
    "#         if benchmodel == 1:\n",
    "#             x1, x2 = fluid.layers.split(\n",
    "#                 input, num_or_sections=[input.shape[1]//2, input.shape[1]//2], dim=1)            \n",
    "# #             x1 = input[:, :(input.shape[1]//2), :, :]\n",
    "# #             x2 = input[:, (input.shape[1]//2):, :, :]\n",
    "            \n",
    "#             conv_pw = self.conv_bn_layer(\n",
    "#                 input=x2, \n",
    "#                 num_filters=oup_inc, \n",
    "#                 filter_size=1, \n",
    "#                 stride=1,\n",
    "#                 padding=0,\n",
    "#                 num_groups=1,\n",
    "#                 if_act=True,\n",
    "#                 name='stage_'+name+'_conv1')\n",
    "\n",
    "#             conv_dw = self.conv_bn_layer(\n",
    "#                 input=conv_pw, \n",
    "#                 num_filters=oup_inc, \n",
    "#                 filter_size=3, \n",
    "#                 stride=stride, \n",
    "#                 padding=1,\n",
    "#                 num_groups=oup_inc, \n",
    "#                 if_act=False,\n",
    "#                 use_cudnn=False,\n",
    "#                 name='stage_'+name+'_conv2')\n",
    "\n",
    "#             conv_linear = self.conv_bn_layer(\n",
    "#                 input=conv_dw, \n",
    "#                 num_filters=oup_inc, \n",
    "#                 filter_size=1, \n",
    "#                 stride=1, \n",
    "#                 padding=0,\n",
    "#                 num_groups=1, \n",
    "#                 if_act=True,\n",
    "#                 name='stage_'+name+'_conv3')\n",
    "            \n",
    "#             out = fluid.layers.concat([x1, conv_linear], axis=1)\n",
    "\n",
    "            \n",
    "#         else:\n",
    "#             #branch1\n",
    "#             conv_dw_1 = self.conv_bn_layer(\n",
    "#                 input=input, \n",
    "#                 num_filters=inp, \n",
    "#                 filter_size=3, \n",
    "#                 stride=stride,\n",
    "#                 padding=1,\n",
    "#                 num_groups=inp,\n",
    "#                 if_act=False,\n",
    "#                 use_cudnn=False,\n",
    "#                 name='stage_'+name+'_conv4')\n",
    "            \n",
    "#             conv_linear_1 = self.conv_bn_layer(\n",
    "#                 input=conv_dw_1, \n",
    "#                 num_filters=oup_inc, \n",
    "#                 filter_size=1, \n",
    "#                 stride=1,\n",
    "#                 padding=0,\n",
    "#                 num_groups=1,\n",
    "#                 if_act=True,\n",
    "#                 name='stage_'+name+'_conv5')\n",
    "            \n",
    "#             #branch2\n",
    "#             conv_pw_2 = self.conv_bn_layer(\n",
    "#                 input=input, \n",
    "#                 num_filters=oup_inc, \n",
    "#                 filter_size=1, \n",
    "#                 stride=1,\n",
    "#                 padding=0,\n",
    "#                 num_groups=1,\n",
    "#                 if_act=True,\n",
    "#                 name='stage_'+name+'_conv1')\n",
    "\n",
    "#             conv_dw_2 = self.conv_bn_layer(\n",
    "#                 input=conv_pw_2, \n",
    "#                 num_filters=oup_inc, \n",
    "#                 filter_size=3, \n",
    "#                 stride=stride, \n",
    "#                 padding=1,\n",
    "#                 num_groups=oup_inc, \n",
    "#                 if_act=False,\n",
    "#                 use_cudnn=False,\n",
    "#                 name='stage_'+name+'_conv2')\n",
    "\n",
    "#             conv_linear_2 = self.conv_bn_layer(\n",
    "#                 input=conv_dw_2, \n",
    "#                 num_filters=oup_inc, \n",
    "#                 filter_size=1, \n",
    "#                 stride=1, \n",
    "#                 padding=0,\n",
    "#                 num_groups=1, \n",
    "#                 if_act=True,\n",
    "#                 name='stage_'+name+'_conv3')\n",
    "#             out = fluid.layers.concat([conv_linear_1, conv_linear_2], axis=1)\n",
    "            \n",
    "#         return self.channel_shuffle(out, 2)\n",
    "\n",
    "# def multilayer_perceptron(input,class_dim): \n",
    "#     # 第一个全连接层，激活函数为ReLU \n",
    "#     hidden1 = fluid.layers.fc(input=input, size=100, act='relu') \n",
    "#     # 第二个全连接层，激活函数为ReLU \n",
    "#     hidden2 = fluid.layers.fc(input=hidden1, size=100, act='relu') \n",
    "#     # 以softmax为激活函数的全连接输出层，输出层的大小必须为数字的个数10 \n",
    "#     prediction = fluid.layers.fc(input=hidden2, size=class_dim, act='softmax',param_attr=ParamAttr(initializer=MSRA(),name='fc_w')) \n",
    "#     return prediction , hidden1\n",
    "\n",
    "# class LeNet(fluid.dygraph.Layer):\n",
    "#     def __init__(self, name_scope,num_classes=1):\n",
    "#         super(LeNet, self).__init__(name_scope)\n",
    "#         name_scope = self.full_name()\n",
    "#         # 创建卷积和池化层块，每个卷积层使用Sigmoid激活函数，后面跟着一个2x2的池化\n",
    "#         self.conv1 = Conv2D(name_scope, num_filters=6, filter_size=5, act='relu')\n",
    "#         self.pool1 = Pool2D(name_scope, pool_size=2, pool_stride=2, pool_type='max')\n",
    "#         self.conv2 = Conv2D(name_scope, num_filters=16, filter_size=5, act='relu')\n",
    "#         self.pool2 = Pool2D(name_scope, pool_size=2, pool_stride=2, pool_type='max')\n",
    "#         # 创建第3个卷积层\n",
    "#         self.conv3 = Conv2D(name_scope, num_filters=120, filter_size=4, act='relu')\n",
    "#         # 创建全连接层，第一个全连接层的输出神经元个数为64， 第二个全连接层输出神经元个数为分裂标签的类别数\n",
    "#         self.fc1 = FC(name_scope, size=64, act='relu')\n",
    "#         self.fc2 = FC(name_scope, size=num_classes,act='softmax',param_attr=ParamAttr(initializer=MSRA(),name='fc_w'))\n",
    "#     # 网络的前向计算过程\n",
    "#     def forward(self, x):\n",
    "#         x1 = self.conv1(x)\n",
    "#         x = self.pool1(x1)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.pool2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x, x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MobileNet():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def name(self):\n",
    "        return 'mobile-net'\n",
    "\n",
    "    def net(self, input, class_dim=1000, scale=1.0):\n",
    "        # conv1: 112x112\n",
    "        input = self.conv_bn_layer(\n",
    "            input,\n",
    "            filter_size=3,\n",
    "            num_filters=int(32 * scale),\n",
    "            stride=2,\n",
    "            padding=1)\n",
    "\n",
    "        conv1 = input\n",
    "        \n",
    "        # 56x56\n",
    "        input = self.depthwise_separable(\n",
    "            input,\n",
    "            num_filters1=32,\n",
    "            num_filters2=64,\n",
    "            num_groups=32,\n",
    "            stride=1,\n",
    "            scale=scale)\n",
    "\n",
    "        input = self.depthwise_separable(\n",
    "            input,\n",
    "            num_filters1=64,\n",
    "            num_filters2=128,\n",
    "            num_groups=64,\n",
    "            stride=2,\n",
    "            scale=scale)\n",
    "\n",
    "        # 28x28\n",
    "        input = self.depthwise_separable(\n",
    "            input,\n",
    "            num_filters1=128,\n",
    "            num_filters2=128,\n",
    "            num_groups=128,\n",
    "            stride=1,\n",
    "            scale=scale)\n",
    "\n",
    "        input = self.depthwise_separable(\n",
    "            input,\n",
    "            num_filters1=128,\n",
    "            num_filters2=256,\n",
    "            num_groups=128,\n",
    "            stride=2,\n",
    "            scale=scale)\n",
    "\n",
    "        # 14x14\n",
    "        input = self.depthwise_separable(\n",
    "            input,\n",
    "            num_filters1=256,\n",
    "            num_filters2=256,\n",
    "            num_groups=256,\n",
    "            stride=1,\n",
    "            scale=scale)\n",
    "\n",
    "        input = self.depthwise_separable(\n",
    "            input,\n",
    "            num_filters1=256,\n",
    "            num_filters2=512,\n",
    "            num_groups=256,\n",
    "            stride=2,\n",
    "            scale=scale)\n",
    "\n",
    "        # 14x14\n",
    "        for i in range(5):\n",
    "            input = self.depthwise_separable(\n",
    "                input,\n",
    "                num_filters1=512,\n",
    "                num_filters2=512,\n",
    "                num_groups=512,\n",
    "                stride=1,\n",
    "                scale=scale)\n",
    "        module1 = input\n",
    "        # 7x7\n",
    "        input = self.depthwise_separable(\n",
    "            input,\n",
    "            num_filters1=512,\n",
    "            num_filters2=1024,\n",
    "            num_groups=512,\n",
    "            stride=2,\n",
    "            scale=scale)\n",
    "\n",
    "        input = self.depthwise_separable(\n",
    "            input,\n",
    "            num_filters1=1024,\n",
    "            num_filters2=1024,\n",
    "            num_groups=1024,\n",
    "            stride=1,\n",
    "            scale=scale)\n",
    "\n",
    "        # class_dim x 1\n",
    "        input = paddle.fluid.layers.conv2d(\n",
    "            input,\n",
    "            num_filters=class_dim,\n",
    "            filter_size=1,\n",
    "            stride=1)\n",
    "\n",
    "        pool = fluid.layers.pool2d(\n",
    "            input=input,\n",
    "            pool_size=0,\n",
    "            pool_stride=1,\n",
    "            pool_type='avg',\n",
    "            global_pooling=True)\n",
    "\n",
    "        output = fluid.layers.fc(input=pool,\n",
    "                              size=class_dim,\n",
    "                              act='softmax', \n",
    "                              param_attr=ParamAttr(name='fc_w_MN',initializer=MSRA()))\n",
    "        \n",
    "        return output, conv1\n",
    "\n",
    "    def conv_bn_layer(self,\n",
    "                      input,\n",
    "                      filter_size,\n",
    "                      num_filters,\n",
    "                      stride,\n",
    "                      padding,\n",
    "                      num_groups=1,\n",
    "                      act='relu',\n",
    "                      use_cudnn=True):\n",
    "        conv = fluid.layers.conv2d(\n",
    "            input=input,\n",
    "            num_filters=num_filters,\n",
    "            filter_size=filter_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=num_groups,\n",
    "            act=None,\n",
    "            use_cudnn=use_cudnn,\n",
    "            param_attr=ParamAttr(initializer=MSRA()),\n",
    "            bias_attr=False)\n",
    "        return fluid.layers.batch_norm(input=conv, act=act)\n",
    "\n",
    "    def depthwise_separable(self, input, num_filters1, num_filters2, num_groups,\n",
    "                            stride, scale):\n",
    "        depthwise_conv = self.conv_bn_layer(\n",
    "            input=input,\n",
    "            filter_size=3,\n",
    "            num_filters=int(num_filters1 * scale),\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            num_groups=int(num_groups * scale),\n",
    "            use_cudnn=True)\n",
    "\n",
    "        pointwise_conv = self.conv_bn_layer(\n",
    "            input=depthwise_conv,\n",
    "            filter_size=1,\n",
    "            num_filters=int(num_filters2 * scale),\n",
    "            stride=1,\n",
    "            padding=0)\n",
    "        return pointwise_conv\n",
    "\n",
    "class AlexNet():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        返回网络名字\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return 'AlexNet'\n",
    "\n",
    "    def net(self, input, class_dim=1000):\n",
    "        stdv = 1.0 / math.sqrt(input.shape[1] * 11 * 11)\n",
    "        layer_name = [\n",
    "            \"conv1\", \"conv2\", \"conv3\", \"conv4\", \"conv5\", \"fc6\", \"fc7\", \"fc8\"\n",
    "        ]\n",
    "        conv1 = fluid.layers.conv2d(\n",
    "            input=input,\n",
    "            num_filters=64,\n",
    "            filter_size=11,\n",
    "            stride=4,\n",
    "            padding=2,\n",
    "            groups=1,\n",
    "            act='relu',\n",
    "            bias_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[0] + \"_offset_AN\"),\n",
    "            param_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[0] + \"_weights_AN\"))\n",
    "        pool1 = fluid.layers.pool2d(\n",
    "            input=conv1,\n",
    "            pool_size=3,\n",
    "            pool_stride=2,\n",
    "            pool_padding=0,\n",
    "            pool_type='max')\n",
    "\n",
    "        stdv = 1.0 / math.sqrt(pool1.shape[1] * 5 * 5)\n",
    "        conv2 = fluid.layers.conv2d(\n",
    "            input=pool1,\n",
    "            num_filters=192,\n",
    "            filter_size=5,\n",
    "            stride=1,\n",
    "            padding=2,\n",
    "            groups=1,\n",
    "            act='relu',\n",
    "            bias_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[1] + \"_offset_AN\"),\n",
    "            param_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[1] + \"_weights_AN\"))\n",
    "        pool2 = fluid.layers.pool2d(\n",
    "            input=conv2,\n",
    "            pool_size=3,\n",
    "            pool_stride=2,\n",
    "            pool_padding=0,\n",
    "            pool_type='max')\n",
    "\n",
    "        stdv = 1.0 / math.sqrt(pool2.shape[1] * 3 * 3)\n",
    "        conv3 = fluid.layers.conv2d(\n",
    "            input=pool2,\n",
    "            num_filters=384,\n",
    "            filter_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            groups=1,\n",
    "            act='relu',\n",
    "            bias_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[2] + \"_offset_AN\"),\n",
    "            param_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[2] + \"_weights_AN\"))\n",
    "\n",
    "        stdv = 1.0 / math.sqrt(conv3.shape[1] * 3 * 3)\n",
    "        conv4 = fluid.layers.conv2d(\n",
    "            input=conv3,\n",
    "            num_filters=256,\n",
    "            filter_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            groups=1,\n",
    "            act='relu',\n",
    "            bias_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[3] + \"_offset_AN\"),\n",
    "            param_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[3] + \"_weights_AN\"))\n",
    "\n",
    "        stdv = 1.0 / math.sqrt(conv4.shape[1] * 3 * 3)\n",
    "        conv5 = fluid.layers.conv2d(\n",
    "            input=conv4,\n",
    "            num_filters=256,\n",
    "            filter_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            groups=1,\n",
    "            act='relu',\n",
    "            bias_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[4] + \"_offset_AN\"),\n",
    "            param_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[4] + \"_weights_AN\"))\n",
    "        pool5 = fluid.layers.pool2d(\n",
    "            input=conv5,\n",
    "            pool_size=3,\n",
    "            pool_stride=2,\n",
    "            pool_padding=0,\n",
    "            pool_type='max')\n",
    "\n",
    "        drop6 = fluid.layers.dropout(x=pool5, dropout_prob=0.5)\n",
    "        stdv = 1.0 / math.sqrt(drop6.shape[1] * drop6.shape[2] *\n",
    "                               drop6.shape[3] * 1.0)\n",
    "\n",
    "        fc6 = fluid.layers.fc(\n",
    "            input=drop6,\n",
    "            size=4096,\n",
    "            act='relu',\n",
    "            bias_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[5] + \"_offset_AN\"),\n",
    "            param_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[5] + \"_weights_AN\"))\n",
    "\n",
    "        drop7 = fluid.layers.dropout(x=fc6, dropout_prob=0.5)\n",
    "        stdv = 1.0 / math.sqrt(drop7.shape[1] * 1.0)\n",
    "\n",
    "        fc7 = fluid.layers.fc(\n",
    "            input=drop7,\n",
    "            size=4096,\n",
    "            act='relu',\n",
    "            bias_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[6] + \"_offset_AN\"),\n",
    "            param_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[6] + \"_weights_AN\"))\n",
    "\n",
    "        stdv = 1.0 / math.sqrt(fc7.shape[1] * 1.0)\n",
    "        out = fluid.layers.fc(\n",
    "            input=fc7,\n",
    "            size=class_dim,\n",
    "            act='softmax',\n",
    "            bias_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=layer_name[7] + \"_offset_AN\"),\n",
    "            param_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name ='fc_w_AN'))\n",
    "        return out, conv1\n",
    "\n",
    "class ShuffleNetV2():\n",
    "    def __init__(self, scale=1.0):\n",
    "        self.scale = scale\n",
    "\n",
    "    def net(self, input, class_dim=1000):\n",
    "        scale = self.scale \n",
    "        stage_repeats = [4, 8, 4]\n",
    "        \n",
    "        if scale == 0.5:\n",
    "            stage_out_channels = [-1, 24,  48,  96, 192, 1024]\n",
    "        elif scale == 1.0:\n",
    "            stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n",
    "        elif scale == 1.5:\n",
    "            stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n",
    "        elif scale == 2.0:\n",
    "            stage_out_channels = [-1, 24, 224, 488, 976, 2048]\n",
    "        elif scale == 8.0:\n",
    "            stage_out_channels = [-1, 48, 896, 1952, 3904, 8192]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"\"\"{} groups is not supported for\n",
    "                       1x1 Grouped Convolutions\"\"\".format(num_groups))\n",
    "\n",
    "        #conv1\n",
    "        \n",
    "        input_channel = stage_out_channels[1]\n",
    "        conv1 = self.conv_bn_layer(input=input, filter_size=3, num_filters=input_channel, padding=1, stride=2,name='stage1_conv')    \n",
    "        pool1 = fluid.layers.pool2d(input=conv1, pool_size=3, pool_stride=2, pool_padding=1, pool_type='max')\n",
    "        conv = pool1\n",
    "        # bottleneck sequences\n",
    "        for idxstage in range(len(stage_repeats)):\n",
    "            numrepeat = stage_repeats[idxstage]\n",
    "            output_channel = stage_out_channels[idxstage+2]\n",
    "            for i in range(numrepeat):\n",
    "                if i == 0:\n",
    "                    conv = self.inverted_residual_unit(input=conv, num_filters=output_channel, stride=2, \n",
    "                                                       benchmodel=2,name=str(idxstage+2)+'_'+str(i+1))\n",
    "                else:\n",
    "                    conv = self.inverted_residual_unit(input=conv, num_filters=output_channel, stride=1, \n",
    "                                                       benchmodel=1,name=str(idxstage+2)+'_'+str(i+1))\n",
    "                \n",
    "        conv_last = self.conv_bn_layer(input=conv, filter_size=1, num_filters=stage_out_channels[-1], \n",
    "                                       padding=0, stride=1, name='conv5')\n",
    "        pool_last = fluid.layers.pool2d(input=conv_last, pool_size=7, pool_stride=1, pool_padding=0, pool_type='avg')\n",
    "\n",
    "\n",
    "        output = fluid.layers.fc(input=pool_last,\n",
    "                                 size=class_dim,\n",
    "                                 act=\"softmax\",\n",
    "                                 param_attr=ParamAttr(initializer=MSRA(),name='fc_w_SN'),\n",
    "                                 bias_attr=ParamAttr(name='fc6_offset_SN'))\n",
    "        return output, conv1\n",
    "\n",
    "    \n",
    "    def conv_bn_layer(self,\n",
    "                  input,\n",
    "                  filter_size,\n",
    "                  num_filters,\n",
    "                  stride,\n",
    "                  padding,\n",
    "                  num_groups=1,\n",
    "                  use_cudnn=True,\n",
    "                  if_act=True,\n",
    "                  name=None):\n",
    "#         print(num_groups)\n",
    "        conv = fluid.layers.conv2d(\n",
    "            input=input,\n",
    "            num_filters=num_filters,\n",
    "            filter_size=filter_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=num_groups,\n",
    "            act=None,\n",
    "            use_cudnn=use_cudnn,\n",
    "            param_attr=ParamAttr(initializer=MSRA(),name=name+'_weights_SN'),\n",
    "            bias_attr=False)\n",
    "        out = int((input.shape[2] - 1)/float(stride) + 1)\n",
    "       # print(input.shape[1],(out, out), num_filters, (filter_size, filter_size), stride, \n",
    "       #       (filter_size - 1) / 2, num_groups, name)\n",
    "        bn_name = name + '_bn'\n",
    "        if if_act:\n",
    "            return fluid.layers.batch_norm(input=conv, act='swish',\n",
    "                                           param_attr = ParamAttr(name=bn_name+\"_scale_SN\"),\n",
    "                                           bias_attr=ParamAttr(name=bn_name+\"_offset_SN\"),\n",
    "                                           moving_mean_name=bn_name + '_mean',\n",
    "                                           moving_variance_name=bn_name + '_variance')\n",
    "        else:\n",
    "            return fluid.layers.batch_norm(input=conv,\n",
    "                                           param_attr = ParamAttr(name=bn_name+\"_scale_SN\"),\n",
    "                                           bias_attr=ParamAttr(name=bn_name+\"_offset_SN\"),\n",
    "                                           moving_mean_name=bn_name + '_mean',\n",
    "                                           moving_variance_name=bn_name + '_variance')\n",
    "\n",
    "      \n",
    "    def channel_shuffle(self, x, groups):\n",
    "        batchsize, num_channels, height, width = x.shape[0], x.shape[1], x.shape[2], x.shape[3]\n",
    "        channels_per_group = num_channels // groups\n",
    "    \n",
    "        # reshape\n",
    "        x = fluid.layers.reshape(x=x, shape=[batchsize, groups, channels_per_group, height, width])\n",
    "\n",
    "        x = fluid.layers.transpose(x=x, perm=[0,2,1,3,4])\n",
    "\n",
    "        # flatten\n",
    "        x = fluid.layers.reshape(x=x, shape=[batchsize, num_channels, height, width])\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def inverted_residual_unit(self, input, num_filters, stride, benchmodel, name=None):\n",
    "        assert stride in [1, 2], \\\n",
    "            \"supported stride are {} but your stride is {}\".format([1,2], stride)\n",
    "            \n",
    "        oup_inc = num_filters//2\n",
    "        inp = input.shape[1]\n",
    "        \n",
    "        if benchmodel == 1:\n",
    "            x1, x2 = fluid.layers.split(\n",
    "                input, num_or_sections=[input.shape[1]//2, input.shape[1]//2], dim=1)            \n",
    "#             x1 = input[:, :(input.shape[1]//2), :, :]\n",
    "#             x2 = input[:, (input.shape[1]//2):, :, :]\n",
    "            \n",
    "            conv_pw = self.conv_bn_layer(\n",
    "                input=x2, \n",
    "                num_filters=oup_inc, \n",
    "                filter_size=1, \n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                num_groups=1,\n",
    "                if_act=True,\n",
    "                name='stage_'+name+'_conv1')\n",
    "\n",
    "            conv_dw = self.conv_bn_layer(\n",
    "                input=conv_pw, \n",
    "                num_filters=oup_inc, \n",
    "                filter_size=3, \n",
    "                stride=stride, \n",
    "                padding=1,\n",
    "                num_groups=oup_inc, \n",
    "                if_act=False,\n",
    "                use_cudnn=False,\n",
    "                name='stage_'+name+'_conv2')\n",
    "\n",
    "            conv_linear = self.conv_bn_layer(\n",
    "                input=conv_dw, \n",
    "                num_filters=oup_inc, \n",
    "                filter_size=1, \n",
    "                stride=1, \n",
    "                padding=0,\n",
    "                num_groups=1, \n",
    "                if_act=True,\n",
    "                name='stage_'+name+'_conv3')\n",
    "            \n",
    "            out = fluid.layers.concat([x1, conv_linear], axis=1)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            #branch1\n",
    "            conv_dw_1 = self.conv_bn_layer(\n",
    "                input=input, \n",
    "                num_filters=inp, \n",
    "                filter_size=3, \n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                num_groups=inp,\n",
    "                if_act=False,\n",
    "                use_cudnn=False,\n",
    "                name='stage_'+name+'_conv4')\n",
    "            \n",
    "            conv_linear_1 = self.conv_bn_layer(\n",
    "                input=conv_dw_1, \n",
    "                num_filters=oup_inc, \n",
    "                filter_size=1, \n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                num_groups=1,\n",
    "                if_act=True,\n",
    "                name='stage_'+name+'_conv5')\n",
    "            \n",
    "            #branch2\n",
    "            conv_pw_2 = self.conv_bn_layer(\n",
    "                input=input, \n",
    "                num_filters=oup_inc, \n",
    "                filter_size=1, \n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                num_groups=1,\n",
    "                if_act=True,\n",
    "                name='stage_'+name+'_conv1')\n",
    "\n",
    "            conv_dw_2 = self.conv_bn_layer(\n",
    "                input=conv_pw_2, \n",
    "                num_filters=oup_inc, \n",
    "                filter_size=3, \n",
    "                stride=stride, \n",
    "                padding=1,\n",
    "                num_groups=oup_inc, \n",
    "                if_act=False,\n",
    "                use_cudnn=False,\n",
    "                name='stage_'+name+'_conv2')\n",
    "\n",
    "            conv_linear_2 = self.conv_bn_layer(\n",
    "                input=conv_dw_2, \n",
    "                num_filters=oup_inc, \n",
    "                filter_size=1, \n",
    "                stride=1, \n",
    "                padding=0,\n",
    "                num_groups=1, \n",
    "                if_act=True,\n",
    "                name='stage_'+name+'_conv3')\n",
    "            out = fluid.layers.concat([conv_linear_1, conv_linear_2], axis=1)\n",
    "            \n",
    "        return self.channel_shuffle(out, 2)\n",
    "\n",
    "\n",
    "class SE_ResNeXt():\n",
    "    def __init__(self, layers=50):\n",
    "        self.params = train_parameters\n",
    "        self.layers = layers\n",
    "\n",
    "    def net(self, input, class_dim=1000):\n",
    "        layers = self.layers\n",
    "        supported_layers = [50, 101, 152]\n",
    "        assert layers in supported_layers, \\\n",
    "            \"supported layers are {} but input layer is {}\".format(supported_layers, layers)\n",
    "        if layers == 50:\n",
    "            cardinality = 32\n",
    "            reduction_ratio = 16\n",
    "            depth = [3, 4, 6, 3]\n",
    "            num_filters = [128, 256, 512, 1024]\n",
    "\n",
    "            conv = self.conv_bn_layer(\n",
    "                input=input,\n",
    "                num_filters=64,\n",
    "                filter_size=7,\n",
    "                stride=2,\n",
    "                act='relu',\n",
    "                name='conv1', )\n",
    "            conv1 = conv\n",
    "            conv = fluid.layers.pool2d(\n",
    "                input=conv,\n",
    "                pool_size=3,\n",
    "                pool_stride=2,\n",
    "                pool_padding=1,\n",
    "                pool_type='max')\n",
    "        elif layers == 101:\n",
    "            cardinality = 32\n",
    "            reduction_ratio = 16\n",
    "            depth = [3, 4, 23, 3]\n",
    "            num_filters = [128, 256, 512, 1024]\n",
    "\n",
    "            conv = self.conv_bn_layer(\n",
    "                input=input,\n",
    "                num_filters=64,\n",
    "                filter_size=7,\n",
    "                stride=2,\n",
    "                act='relu',\n",
    "                name=\"conv1\", )\n",
    "            conv = fluid.layers.pool2d(\n",
    "                input=conv,\n",
    "                pool_size=3,\n",
    "                pool_stride=2,\n",
    "                pool_padding=1,\n",
    "                pool_type='max')\n",
    "        elif layers == 152:\n",
    "            cardinality = 64\n",
    "            reduction_ratio = 16\n",
    "            depth = [3, 8, 36, 3]\n",
    "            num_filters = [128, 256, 512, 1024]\n",
    "\n",
    "            conv = self.conv_bn_layer(\n",
    "                input=input,\n",
    "                num_filters=64,\n",
    "                filter_size=3,\n",
    "                stride=2,\n",
    "                act='relu',\n",
    "                name='conv1')\n",
    "            conv = self.conv_bn_layer(\n",
    "                input=conv,\n",
    "                num_filters=64,\n",
    "                filter_size=3,\n",
    "                stride=1,\n",
    "                act='relu',\n",
    "                name='conv2')\n",
    "            conv = self.conv_bn_layer(\n",
    "                input=conv,\n",
    "                num_filters=128,\n",
    "                filter_size=3,\n",
    "                stride=1,\n",
    "                act='relu',\n",
    "                name='conv3')\n",
    "            conv = fluid.layers.pool2d(\n",
    "                input=conv, pool_size=3, pool_stride=2, pool_padding=1, \\\n",
    "                pool_type='max')\n",
    "        n = 1 if layers == 50 or layers == 101 else 3\n",
    "        for block in range(len(depth)):\n",
    "            n += 1\n",
    "            for i in range(depth[block]):\n",
    "                conv = self.bottleneck_block(\n",
    "                    input=conv,\n",
    "                    num_filters=num_filters[block],\n",
    "                    stride=2 if i == 0 and block != 0 else 1,\n",
    "                    cardinality=cardinality,\n",
    "                    reduction_ratio=reduction_ratio,\n",
    "                    name=str(n) + '_' + str(i + 1))\n",
    "\n",
    "        pool = fluid.layers.pool2d(\n",
    "            input=conv, pool_size=7, pool_type='avg', global_pooling=True)\n",
    "        drop = fluid.layers.dropout(\n",
    "            x=pool, dropout_prob=0.5, seed=self.params['dropout_seed'])\n",
    "        stdv = 1.0 / math.sqrt(drop.shape[1] * 1.0)\n",
    "        out = fluid.layers.fc(\n",
    "            input=drop,\n",
    "            size=class_dim,\n",
    "            act=\"softmax\",\n",
    "            param_attr=ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name='fc_w_SE_Res'),\n",
    "            bias_attr=ParamAttr(name='fc6_offset'))\n",
    "        return out, conv1\n",
    "\n",
    "    def shortcut(self, input, ch_out, stride, name):\n",
    "        ch_in = input.shape[1]\n",
    "        if ch_in != ch_out or stride != 1:\n",
    "            filter_size = 1\n",
    "            return self.conv_bn_layer(\n",
    "                input, ch_out, filter_size, stride, name='conv' + name + '_prj')\n",
    "        else:\n",
    "            return input\n",
    "\n",
    "    def bottleneck_block(self,\n",
    "                         input,\n",
    "                         num_filters,\n",
    "                         stride,\n",
    "                         cardinality,\n",
    "                         reduction_ratio,\n",
    "                         name=None):\n",
    "        conv0 = self.conv_bn_layer(\n",
    "            input=input,\n",
    "            num_filters=num_filters,\n",
    "            filter_size=1,\n",
    "            act='relu',\n",
    "            name='conv' + name + '_x1')\n",
    "        conv1 = self.conv_bn_layer(\n",
    "            input=conv0,\n",
    "            num_filters=num_filters,\n",
    "            filter_size=3,\n",
    "            stride=stride,\n",
    "            groups=cardinality,\n",
    "            act='relu',\n",
    "            name='conv' + name + '_x2')\n",
    "        conv2 = self.conv_bn_layer(\n",
    "            input=conv1,\n",
    "            num_filters=num_filters * 2,\n",
    "            filter_size=1,\n",
    "            act=None,\n",
    "            name='conv' + name + '_x3')\n",
    "        scale = self.squeeze_excitation(\n",
    "            input=conv2,\n",
    "            num_channels=num_filters * 2,\n",
    "            reduction_ratio=reduction_ratio,\n",
    "            name='fc' + name)\n",
    "\n",
    "        short = self.shortcut(input, num_filters * 2, stride, name=name)\n",
    "\n",
    "        return fluid.layers.elementwise_add(x=short, y=scale, act='relu')\n",
    "\n",
    "    def conv_bn_layer(self,\n",
    "                      input,\n",
    "                      num_filters,\n",
    "                      filter_size,\n",
    "                      stride=1,\n",
    "                      groups=1,\n",
    "                      act=None,\n",
    "                      name=None):\n",
    "        conv = fluid.layers.conv2d(\n",
    "            input=input,\n",
    "            num_filters=num_filters,\n",
    "            filter_size=filter_size,\n",
    "            stride=stride,\n",
    "            padding=(filter_size - 1) // 2,\n",
    "            groups=groups,\n",
    "            act=None,\n",
    "            bias_attr=False,\n",
    "            param_attr=ParamAttr(name=name + '_weights'), )\n",
    "        bn_name = name + \"_bn\"\n",
    "        return fluid.layers.batch_norm(\n",
    "            input=conv,\n",
    "            act=act,\n",
    "            param_attr=ParamAttr(name=bn_name + '_scale'),\n",
    "            bias_attr=ParamAttr(bn_name + '_offset'),\n",
    "            moving_mean_name=bn_name + '_mean',\n",
    "            moving_variance_name=bn_name + '_variance')\n",
    "\n",
    "    def squeeze_excitation(self,\n",
    "                           input,\n",
    "                           num_channels,\n",
    "                           reduction_ratio,\n",
    "                           name=None):\n",
    "        pool = fluid.layers.pool2d(\n",
    "            input=input, pool_size=0, pool_type='avg', global_pooling=True)\n",
    "        stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)\n",
    "        squeeze = fluid.layers.fc(\n",
    "            input=pool,\n",
    "            size=num_channels // reduction_ratio,\n",
    "            act='relu',\n",
    "            param_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=name + '_sqz_weights'),\n",
    "            bias_attr=ParamAttr(name=name + '_sqz_offset'))\n",
    "        stdv = 1.0 / math.sqrt(squeeze.shape[1] * 1.0)\n",
    "        excitation = fluid.layers.fc(\n",
    "            input=squeeze,\n",
    "            size=num_channels,\n",
    "            act='sigmoid',\n",
    "            param_attr=fluid.param_attr.ParamAttr(\n",
    "                initializer=fluid.initializer.Uniform(-stdv, stdv),\n",
    "                name=name + '_exc_weights'),\n",
    "            bias_attr=ParamAttr(name=name + '_exc_offset'))\n",
    "        scale = fluid.layers.elementwise_mul(x=input, y=excitation, axis=0)\n",
    "        return scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# target_size = [3, 256, 256]     \n",
    "# mean_rgb = [127.5, 127.5, 127.5]     \n",
    "# data_dir = path     \n",
    "# eval_file = \"test.txt\"     \n",
    "# save_test_dir \"./test-model\"\n",
    "# # print(fetch_targets)     \n",
    "     \n",
    "     \n",
    "# def crop_image(img, target_size):     \n",
    "#     width, height = img.size\n",
    "#     p = min(target_size[2] / width, target_size[1] / height)\n",
    "#     resized_h = int(height * p)\n",
    "#     resized_w = int(width * p)\n",
    "#     img = img.resize((resized_w, resized_h), Image.BILINEAR)\n",
    "#     w_start = (resized_w - target_size[2]) / 2     \n",
    "#     h_start = (resized_h - target_size[1]) / 2     \n",
    "#     w_end = w_start + target_size[2]     \n",
    "#     h_end = h_start + target_size[1]     \n",
    "#     img = img.crop((w_start, h_start, w_end, h_end))     \n",
    "#     return img     \n",
    "\n",
    "\n",
    "# def resize_img(img, target_size):     \n",
    "#     ret = img.resize((target_size[1], target_size[2]), Image.BILINEAR)     \n",
    "#     return ret     \n",
    "\n",
    "\n",
    "# def read_image(img_path):     \n",
    "#     img = Image.open(img_path)     \n",
    "#     if img.mode != 'RGB':     \n",
    "#         img = img.convert('RGB')     \n",
    "#     # img = crop_image(img, target_size)\n",
    "#     img = resize_img(img, target_size)\n",
    "#     img = np.array(img).astype('float32')     \n",
    "#     img -= mean_rgb     \n",
    "#     img = img.transpose((2, 0, 1))  # HWC to CHW     \n",
    "#     img *= 0.007843     \n",
    "#     img = img[np.newaxis,:]     \n",
    "#     return img     \n",
    "     \n",
    "     \n",
    "# def infer_test(image_path):\n",
    "#     use_gpu = True     \n",
    "#     place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()     \n",
    "#     exe = fluid.Executor(place)          \n",
    "#     [inference_program, feed_target_names, fetch_targets] = fluid.io.load_inference_model(dirname=save_test_dir, executor=exe) \n",
    "#     tensor_img = read_image(image_path)     \n",
    "#     label = exe.run(inference_program, feed={feed_target_names[0]: tensor_img}, fetch_list=fetch_targets)     \n",
    "#     return np.argmax(label)     \n",
    "     \n",
    "     \n",
    "# def test_all():     \n",
    "#     eval_file_path = os.path.join(data_dir, eval_file)     \n",
    "#     total_count = 0     \n",
    "#     right_count = 0     \n",
    "#     with codecs.open(eval_file_path, encoding='utf-8') as flist:      \n",
    "#         lines = [line.strip() for line in flist]     \n",
    "#         t1 = time.time()     \n",
    "#         for line in lines:     \n",
    "#             total_count += 1     \n",
    "#             parts = line.strip().split()     \n",
    "#             result = infer_test(parts[0])     \n",
    "#             # print(\"infer result:{0} answer:{1}\".format(result, parts[1]))     \n",
    "#             if str(result) == parts[1]:     \n",
    "#                 right_count += 1     \n",
    "#         period = time.time() - t1  \n",
    "#         test_acc =  right_count / total_count  \n",
    "#         print(\"total eval count:{0} cost time:{1} predict accuracy:{2}\".format(total_count, \"%2.2f sec\" % period, right_count / total_count))     \n",
    "#     return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "import numpy as np\n",
    "from paddle.fluid.dygraph.nn import Conv2D, Pool2D, FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-17 20:14:56,167-INFO: current pass: 0, start read image\n",
      "2020-09-17 20:14:56,167 - <ipython-input-11-19428acb2f4e>[line:119] - INFO: current pass: 0, start read image\n",
      "2020-09-17 20:15:05,862-INFO: Pass 0, trainbatch 10, loss 2.8481287956237793, acc1 0.125, time 0.58 sec\n",
      "2020-09-17 20:15:05,862 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 0, trainbatch 10, loss 2.8481287956237793, acc1 0.125, time 0.58 sec\n",
      "2020-09-17 20:15:13,573-INFO: Pass 0, trainbatch 20, loss 2.672089099884033, acc1 0.0625, time 0.57 sec\n",
      "2020-09-17 20:15:13,573 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 0, trainbatch 20, loss 2.672089099884033, acc1 0.0625, time 0.57 sec\n",
      "2020-09-17 20:15:20,461-INFO: Pass 0, trainbatch 30, loss 2.5238914489746094, acc1 0.3125, time 0.59 sec\n",
      "2020-09-17 20:15:20,461 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 0, trainbatch 30, loss 2.5238914489746094, acc1 0.3125, time 0.59 sec\n",
      "2020-09-17 20:15:29,869-INFO: Pass 0, trainbatch 40, loss 2.7504284381866455, acc1 0.1875, time 0.57 sec\n",
      "2020-09-17 20:15:29,869 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 0, trainbatch 40, loss 2.7504284381866455, acc1 0.1875, time 0.57 sec\n",
      "2020-09-17 20:15:36,983-INFO: Pass 0, trainbatch 50, loss 2.35447359085083, acc1 0.3125, time 0.59 sec\n",
      "2020-09-17 20:15:36,983 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 0, trainbatch 50, loss 2.35447359085083, acc1 0.3125, time 0.59 sec\n",
      "2020-09-17 20:15:36,985-INFO: temp save 50 batch train result, current acc1 0.3125\n",
      "2020-09-17 20:15:36,985 - <ipython-input-11-19428acb2f4e>[line:210] - INFO: temp save 50 batch train result, current acc1 0.3125\n",
      "2020-09-17 20:15:48,108-INFO: Pass 0, trainbatch 60, loss 2.267442464828491, acc1 0.375, time 0.59 sec\n",
      "2020-09-17 20:15:48,108 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 0, trainbatch 60, loss 2.267442464828491, acc1 0.375, time 0.59 sec\n",
      "2020-09-17 20:15:57,714-INFO: test result:Pass 0, acc1 0.4166666567325592, time 0.11 sec\n",
      "2020-09-17 20:15:57,714 - <ipython-input-11-19428acb2f4e>[line:235] - INFO: test result:Pass 0, acc1 0.4166666567325592, time 0.11 sec\n",
      "2020-09-17 20:15:57,716-INFO: current pass: 1, start read image\n",
      "2020-09-17 20:15:57,716 - <ipython-input-11-19428acb2f4e>[line:119] - INFO: current pass: 1, start read image\n",
      "2020-09-17 20:16:07,264-INFO: Pass 1, trainbatch 10, loss 2.095529079437256, acc1 0.40625, time 0.58 sec\n",
      "2020-09-17 20:16:07,264 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 1, trainbatch 10, loss 2.095529079437256, acc1 0.40625, time 0.58 sec\n",
      "2020-09-17 20:16:14,087-INFO: Pass 1, trainbatch 20, loss 2.683195114135742, acc1 0.15625, time 0.58 sec\n",
      "2020-09-17 20:16:14,087 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 1, trainbatch 20, loss 2.683195114135742, acc1 0.15625, time 0.58 sec\n",
      "2020-09-17 20:16:20,827-INFO: Pass 1, trainbatch 30, loss 2.4660727977752686, acc1 0.25, time 0.57 sec\n",
      "2020-09-17 20:16:20,827 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 1, trainbatch 30, loss 2.4660727977752686, acc1 0.25, time 0.57 sec\n",
      "2020-09-17 20:16:26,300-INFO: temp save 100 batch train result, current acc1 0.28125\n",
      "2020-09-17 20:16:26,300 - <ipython-input-11-19428acb2f4e>[line:210] - INFO: temp save 100 batch train result, current acc1 0.28125\n",
      "2020-09-17 20:16:35,093-INFO: Pass 1, trainbatch 40, loss 2.3758342266082764, acc1 0.34375, time 0.57 sec\n",
      "2020-09-17 20:16:35,093 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 1, trainbatch 40, loss 2.3758342266082764, acc1 0.34375, time 0.57 sec\n",
      "2020-09-17 20:16:41,906-INFO: Pass 1, trainbatch 50, loss 2.1837644577026367, acc1 0.3125, time 0.58 sec\n",
      "2020-09-17 20:16:41,906 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 1, trainbatch 50, loss 2.1837644577026367, acc1 0.3125, time 0.58 sec\n",
      "2020-09-17 20:16:49,454-INFO: Pass 1, trainbatch 60, loss 2.4601986408233643, acc1 0.375, time 0.86 sec\n",
      "2020-09-17 20:16:49,454 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 1, trainbatch 60, loss 2.4601986408233643, acc1 0.375, time 0.86 sec\n",
      "2020-09-17 20:16:59,117-INFO: test result:Pass 1, acc1 0.0833333358168602, time 0.10 sec\n",
      "2020-09-17 20:16:59,117 - <ipython-input-11-19428acb2f4e>[line:235] - INFO: test result:Pass 1, acc1 0.0833333358168602, time 0.10 sec\n",
      "2020-09-17 20:16:59,119-INFO: current pass: 2, start read image\n",
      "2020-09-17 20:16:59,119 - <ipython-input-11-19428acb2f4e>[line:119] - INFO: current pass: 2, start read image\n",
      "2020-09-17 20:17:08,151-INFO: Pass 2, trainbatch 10, loss 2.3595681190490723, acc1 0.21875, time 0.55 sec\n",
      "2020-09-17 20:17:08,151 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 2, trainbatch 10, loss 2.3595681190490723, acc1 0.21875, time 0.55 sec\n",
      "2020-09-17 20:17:13,348-INFO: temp save 150 batch train result, current acc1 0.25\n",
      "2020-09-17 20:17:13,348 - <ipython-input-11-19428acb2f4e>[line:210] - INFO: temp save 150 batch train result, current acc1 0.25\n",
      "2020-09-17 20:17:19,343-INFO: Pass 2, trainbatch 20, loss 2.0332021713256836, acc1 0.46875, time 0.87 sec\n",
      "2020-09-17 20:17:19,343 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 2, trainbatch 20, loss 2.0332021713256836, acc1 0.46875, time 0.87 sec\n",
      "2020-09-17 20:17:26,212-INFO: Pass 2, trainbatch 30, loss 2.012796640396118, acc1 0.375, time 0.57 sec\n",
      "2020-09-17 20:17:26,212 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 2, trainbatch 30, loss 2.012796640396118, acc1 0.375, time 0.57 sec\n",
      "2020-09-17 20:17:35,213-INFO: Pass 2, trainbatch 40, loss 2.2199466228485107, acc1 0.21875, time 0.54 sec\n",
      "2020-09-17 20:17:35,213 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 2, trainbatch 40, loss 2.2199466228485107, acc1 0.21875, time 0.54 sec\n",
      "2020-09-17 20:17:42,252-INFO: Pass 2, trainbatch 50, loss 1.949549913406372, acc1 0.40625, time 0.81 sec\n",
      "2020-09-17 20:17:42,252 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 2, trainbatch 50, loss 1.949549913406372, acc1 0.40625, time 0.81 sec\n",
      "2020-09-17 20:17:48,679-INFO: Pass 2, trainbatch 60, loss 2.191387891769409, acc1 0.3125, time 0.54 sec\n",
      "2020-09-17 20:17:48,679 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 2, trainbatch 60, loss 2.191387891769409, acc1 0.3125, time 0.54 sec\n",
      "2020-09-17 20:17:58,150-INFO: test result:Pass 2, acc1 0.4166666567325592, time 0.10 sec\n",
      "2020-09-17 20:17:58,150 - <ipython-input-11-19428acb2f4e>[line:235] - INFO: test result:Pass 2, acc1 0.4166666567325592, time 0.10 sec\n",
      "2020-09-17 20:17:58,152-INFO: current pass: 3, start read image\n",
      "2020-09-17 20:17:58,152 - <ipython-input-11-19428acb2f4e>[line:119] - INFO: current pass: 3, start read image\n",
      "2020-09-17 20:18:01,988-INFO: temp save 200 batch train result, current acc1 0.40625\n",
      "2020-09-17 20:18:01,988 - <ipython-input-11-19428acb2f4e>[line:210] - INFO: temp save 200 batch train result, current acc1 0.40625\n",
      "2020-09-17 20:18:11,958-INFO: Pass 3, trainbatch 10, loss 1.7311012744903564, acc1 0.46875, time 0.58 sec\n",
      "2020-09-17 20:18:11,958 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 3, trainbatch 10, loss 1.7311012744903564, acc1 0.46875, time 0.58 sec\n",
      "2020-09-17 20:18:19,224-INFO: Pass 3, trainbatch 20, loss 2.287726879119873, acc1 0.28125, time 0.58 sec\n",
      "2020-09-17 20:18:19,224 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 3, trainbatch 20, loss 2.287726879119873, acc1 0.28125, time 0.58 sec\n",
      "2020-09-17 20:18:26,140-INFO: Pass 3, trainbatch 30, loss 2.305920124053955, acc1 0.25, time 0.59 sec\n",
      "2020-09-17 20:18:26,140 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 3, trainbatch 30, loss 2.305920124053955, acc1 0.25, time 0.59 sec\n",
      "2020-09-17 20:18:35,687-INFO: Pass 3, trainbatch 40, loss 2.0545239448547363, acc1 0.34375, time 0.85 sec\n",
      "2020-09-17 20:18:35,687 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 3, trainbatch 40, loss 2.0545239448547363, acc1 0.34375, time 0.85 sec\n",
      "2020-09-17 20:18:42,632-INFO: Pass 3, trainbatch 50, loss 2.1911191940307617, acc1 0.375, time 0.59 sec\n",
      "2020-09-17 20:18:42,632 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 3, trainbatch 50, loss 2.1911191940307617, acc1 0.375, time 0.59 sec\n",
      "2020-09-17 20:18:43,954-INFO: temp save 250 batch train result, current acc1 0.53125\n",
      "2020-09-17 20:18:43,954 - <ipython-input-11-19428acb2f4e>[line:210] - INFO: temp save 250 batch train result, current acc1 0.53125\n",
      "2020-09-17 20:18:53,897-INFO: Pass 3, trainbatch 60, loss 2.4231936931610107, acc1 0.28125, time 0.57 sec\n",
      "2020-09-17 20:18:53,897 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 3, trainbatch 60, loss 2.4231936931610107, acc1 0.28125, time 0.57 sec\n",
      "2020-09-17 20:19:03,637-INFO: test result:Pass 3, acc1 0.3333333432674408, time 0.10 sec\n",
      "2020-09-17 20:19:03,637 - <ipython-input-11-19428acb2f4e>[line:235] - INFO: test result:Pass 3, acc1 0.3333333432674408, time 0.10 sec\n",
      "2020-09-17 20:19:03,639-INFO: current pass: 4, start read image\n",
      "2020-09-17 20:19:03,639 - <ipython-input-11-19428acb2f4e>[line:119] - INFO: current pass: 4, start read image\n",
      "2020-09-17 20:19:12,944-INFO: Pass 4, trainbatch 10, loss 1.7907891273498535, acc1 0.5, time 0.57 sec\n",
      "2020-09-17 20:19:12,944 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 4, trainbatch 10, loss 1.7907891273498535, acc1 0.5, time 0.57 sec\n",
      "2020-09-17 20:19:19,474-INFO: Pass 4, trainbatch 20, loss 1.905580759048462, acc1 0.5, time 0.54 sec\n",
      "2020-09-17 20:19:19,474 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 4, trainbatch 20, loss 1.905580759048462, acc1 0.5, time 0.54 sec\n",
      "2020-09-17 20:19:26,007-INFO: Pass 4, trainbatch 30, loss 1.8891586065292358, acc1 0.40625, time 0.54 sec\n",
      "2020-09-17 20:19:26,007 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 4, trainbatch 30, loss 1.8891586065292358, acc1 0.40625, time 0.54 sec\n",
      "2020-09-17 20:19:32,632-INFO: temp save 300 batch train result, current acc1 0.40625\n",
      "2020-09-17 20:19:32,632 - <ipython-input-11-19428acb2f4e>[line:210] - INFO: temp save 300 batch train result, current acc1 0.40625\n",
      "2020-09-17 20:19:40,299-INFO: Pass 4, trainbatch 40, loss 2.2132253646850586, acc1 0.28125, time 0.54 sec\n",
      "2020-09-17 20:19:40,299 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 4, trainbatch 40, loss 2.2132253646850586, acc1 0.28125, time 0.54 sec\n",
      "2020-09-17 20:19:46,927-INFO: Pass 4, trainbatch 50, loss 1.9617745876312256, acc1 0.46875, time 0.54 sec\n",
      "2020-09-17 20:19:46,927 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 4, trainbatch 50, loss 1.9617745876312256, acc1 0.46875, time 0.54 sec\n",
      "2020-09-17 20:19:53,713-INFO: Pass 4, trainbatch 60, loss 2.2299001216888428, acc1 0.25, time 0.56 sec\n",
      "2020-09-17 20:19:53,713 - <ipython-input-11-19428acb2f4e>[line:188] - INFO: Pass 4, trainbatch 60, loss 2.2299001216888428, acc1 0.25, time 0.56 sec\n",
      "2020-09-17 20:20:03,330-INFO: test result:Pass 4, acc1 0.25, time 0.11 sec\n",
      "2020-09-17 20:20:03,330 - <ipython-input-11-19428acb2f4e>[line:235] - INFO: test result:Pass 4, acc1 0.25, time 0.11 sec\n",
      "2020-09-17 20:20:03,332-INFO: training till last epcho, end training\n",
      "2020-09-17 20:20:03,332 - <ipython-input-11-19428acb2f4e>[line:246] - INFO: training till last epcho, end training\n"
     ]
    }
   ],
   "source": [
    "def train(): \r\n",
    "    train_prog = fluid.Program() \r\n",
    "    train_startup = fluid.Program() \r\n",
    "    logger.info(\"create prog success\") \r\n",
    "    logger.info(\"train config: %s\", str(train_parameters)) \r\n",
    "    logger.info(\"build input custom reader and data feeder\") \r\n",
    "    \r\n",
    "    #图像顺序随机\r\n",
    "    # image_shuffle()\r\n",
    "    \r\n",
    "    file_list = os.path.join(train_parameters['data_dir'], \"train.txt\") \r\n",
    "    test_file_list = os.path.join(train_parameters['data_dir'], \"test.txt\") \r\n",
    "    mode = train_parameters['mode'] \r\n",
    "    batch_reader = paddle.batch(custom_image_reader(file_list, train_parameters['data_dir'], mode), \r\n",
    "                                batch_size=train_parameters['train_batch_size'], \r\n",
    "                                drop_last=False) \r\n",
    "    batch_reader = paddle.reader.shuffle(batch_reader, train_parameters['train_batch_size']) \r\n",
    "    test_batch_reader = paddle.batch(custom_image_reader(test_file_list, train_parameters['data_dir'], mode), \r\n",
    "                                batch_size=train_parameters['train_batch_size'], \r\n",
    "                                drop_last=False)    #test\r\n",
    "    place = fluid.CUDAPlace(0) if train_parameters['use_gpu'] else fluid.CPUPlace() \r\n",
    "    # 定义输入数据的占位符 \r\n",
    "    img = fluid.layers.data(name='img', shape=train_parameters['input_size'], dtype='float32') \r\n",
    "    label = fluid.layers.data(name='label', shape=[1], dtype='int64') \r\n",
    "    feeder = fluid.DataFeeder(feed_list=[img, label], place=place) \r\n",
    " \r\n",
    "    # 选取不同的网络 \r\n",
    "    logger.info(\"build newwork\") \r\n",
    "    # # model = DenseNet(121, train_parameters['dropout_prob']) \r\n",
    "    # # model = ResNet(layers=50)\r\n",
    "    # model = VGGNet(layers=16)\r\n",
    "    # model = CNN()\r\n",
    "    # model = MobileNet() #输出全联接层权重获取出错\r\n",
    "    # model = AlexNet()\r\n",
    "    # # model = DistResNet()\r\n",
    "    # # model = InceptionV4()\r\n",
    "    # # model = SE_ResNeXt()\r\n",
    "    # # model = DPN()\r\n",
    "    # model = ShuffleNetV2()\r\n",
    "    \r\n",
    "    # out, conv1 = model.net(\r\n",
    "    #     input=img, \r\n",
    "    #     class_dim=train_parameters['class_dim']) \r\n",
    "\r\n",
    "    # out,conv1 = multilayer_perceptron(input=img,class_dim=train_parameters['class_dim'])\r\n",
    "\r\n",
    "    # model = LeNet(name_scope = \"LeNet\",num_classes=train_parameters['class_dim'])\r\n",
    "    # out, conv1 = model.forward(img)\r\n",
    "    model_SE_Res = SE_ResNeXt()\r\n",
    "    out_SE_Res, conv1_SE_Res = model_SE_Res.net(\r\n",
    "        input=img, \r\n",
    "        class_dim=train_parameters['class_dim'])\r\n",
    "    \r\n",
    "    model_SN = ShuffleNetV2()\r\n",
    "    out_SN, conv1_SN = model_SN.net(\r\n",
    "        input=img, \r\n",
    "        class_dim=train_parameters['class_dim'])\r\n",
    "\r\n",
    "    model_MN = MobileNet() \r\n",
    "    out_MN, conv1_MN = model_MN.net(\r\n",
    "        input=img, \r\n",
    "        class_dim=train_parameters['class_dim'])\r\n",
    "    \r\n",
    "    out = (9*out_SN + out_MN +4*out_SE_Res)*0.07142\r\n",
    "    # conv1 = conv1_SN\r\n",
    "    \r\n",
    "    cost = fluid.layers.cross_entropy(input=out, label=label) \r\n",
    "    avg_cost = fluid.layers.mean(x=cost) \r\n",
    "    acc_top1 = fluid.layers.accuracy(input=out, label=label, k=1) \r\n",
    "    test_program = fluid.default_main_program().clone(for_test=True)\r\n",
    "    optimizer = optimizer_rms_setting() \r\n",
    "    optimizer.minimize(avg_cost) \r\n",
    "    exe = fluid.Executor(place) \r\n",
    "    main_program = fluid.default_main_program() \r\n",
    "    exe.run(fluid.default_startup_program()) \r\n",
    "    param_name = 'fc_w_SN'\r\n",
    "    #conv1 = 'conv1'\r\n",
    "    # train_fetch_list = [avg_cost.name, acc_top1.name, out.name, conv1] \r\n",
    "    train_fetch_list = [avg_cost.name, acc_top1.name, out.name, conv1_SN, param_name] \r\n",
    "    load_params(exe, main_program) \r\n",
    "    #初始化visualDL\r\n",
    "    \r\n",
    "    # create VisualDL logger and directory\r\n",
    "    logdir = \"./log\"\r\n",
    "    logwriter = LogWriter(logdir, sync_cycle=1)\r\n",
    "    # create 'train' run\r\n",
    "    with logwriter.mode(\"train\") as writer:\r\n",
    "        # create 'loss' scalar tag to keep track of loss function\r\n",
    "        loss_scalar = writer.scalar(\"loss\")\r\n",
    "    with logwriter.mode(\"train\") as writer:\r\n",
    "        acc_scalar = writer.scalar(\"acc\")\r\n",
    "    with logwriter.mode(\"train\") as writer:\r\n",
    "        test_acc_scalar = writer.scalar(\"test_acc\")\r\n",
    "    num_samples = 4\r\n",
    "    with logwriter.mode(\"train\") as writer:\r\n",
    "        conv_image = writer.image(\"conv_image\", num_samples, 1) #show 4 samples for every 1 step\r\n",
    "        input_image = writer.image(\"input_image\", num_samples, 1)\r\n",
    "    with logwriter.mode(\"train\") as writer:\r\n",
    "        param1_histgram = writer.histogram(\"param1\", 50) #100 buckets, e.g 100 data sets in a histograms\r\n",
    "    \r\n",
    " \r\n",
    "    all_train_iter=0\r\n",
    "    \r\n",
    "    # 训练循环主体 \r\n",
    "    stop_strategy = train_parameters['early_stop'] \r\n",
    "    successive_limit = stop_strategy['successive_limit'] \r\n",
    "    sample_freq = stop_strategy['sample_frequency'] \r\n",
    "    good_acc1 = stop_strategy['good_acc1'] \r\n",
    "    successive_count = 0 \r\n",
    "    stop_train = False \r\n",
    "    total_batch_count = 0 \r\n",
    "\r\n",
    "    step = 0\r\n",
    "    sample_num = 0\r\n",
    "    test_step = 0\r\n",
    "    data_shape = train_parameters[\"input_size\"]\r\n",
    "    \r\n",
    "    for pass_id in range(train_parameters[\"num_epochs\"]): \r\n",
    "        logger.info(\"current pass: %d, start read image\", pass_id) \r\n",
    "        batch_id = 0 \r\n",
    "        \r\n",
    "        for step_id, data in enumerate(batch_reader()): \r\n",
    "            main_program = fluid.default_main_program() \r\n",
    "            t1 = time.time() \r\n",
    "            # loss, acc1, pred_ot, conv1_out = exe.run(main_program, \r\n",
    "            #                              feed=feeder.feed(data), \r\n",
    "            #                              fetch_list=train_fetch_list) \r\n",
    "            loss, acc1, pred_ot, conv1_out,param1 = exe.run(main_program, \r\n",
    "                                          feed=feeder.feed(data), \r\n",
    "                                          fetch_list=train_fetch_list)\r\n",
    "            \r\n",
    "            t2 = time.time() \r\n",
    "            batch_id += 1 \r\n",
    "            total_batch_count += 1 \r\n",
    "            period = t2 - t1 \r\n",
    "            \r\n",
    "            #acc和loss图像数据提取\r\n",
    "            all_train_iter=all_train_iter+1\r\n",
    "            all_train_iters.append(all_train_iter)\r\n",
    "            all_train_costs.append(loss[0])\r\n",
    "            all_train_accs.append(acc1[0])\r\n",
    "            \r\n",
    "            #Visual DL提取\r\n",
    "            step += 1\r\n",
    "            loss_scalar.add_record(step, loss[0])\r\n",
    "            acc_scalar.add_record(step, acc1[0])\r\n",
    "            param1_histgram.add_record(step, param1.flatten())\r\n",
    "            \r\n",
    "            # start picking sample from beginning\r\n",
    "            if sample_num == 0:\r\n",
    "                input_image.start_sampling()\r\n",
    "                conv_image.start_sampling()\r\n",
    " \r\n",
    "            idx1 = input_image.is_sample_taken()\r\n",
    "            idx2 = conv_image.is_sample_taken()\r\n",
    "            assert idx1 == idx2\r\n",
    "            idx = idx1\r\n",
    "            if idx != -1:\r\n",
    "                image_data = data[0][0]\r\n",
    "                # reshape the image to 32x32 and 3 channels\r\n",
    "                input_image_data = np.transpose(\r\n",
    "                    image_data.reshape(data_shape), axes=[1, 2, 0])\r\n",
    "                # add sample to VisualDL Image Writer to view input image\r\n",
    "                input_image.set_sample(idx, input_image_data.shape,\r\n",
    "                                       input_image_data.flatten())\r\n",
    "                # data1.append(input_image_data)\r\n",
    "               \r\n",
    "                conv_image_data = conv1_out\r\n",
    "                # add sample to view conv image\r\n",
    "                # conv_image.set_sample(idx, conv_image_data.shape,\r\n",
    "                #                       conv_image_data.flatten())\r\n",
    "                # convdata.append(conv_image_data)\r\n",
    "                \r\n",
    "                sample_num += 1\r\n",
    "                # when we have enough samples, call finish sampling()\r\n",
    "                if sample_num % num_samples == 0:\r\n",
    "                    input_image.finish_sampling()\r\n",
    "                    conv_image.finish_sampling()\r\n",
    "                    sample_num = 0\r\n",
    "\r\n",
    "            \r\n",
    "            loss = np.mean(np.array(loss)) \r\n",
    "            acc1 = np.mean(np.array(acc1)) \r\n",
    "            \r\n",
    "            \r\n",
    "            if batch_id % 10 == 0: \r\n",
    "                logger.info(\"Pass {0}, trainbatch {1}, loss {2}, acc1 {3}, time {4}\".format(pass_id, batch_id, loss, acc1, \r\n",
    "                                                                                            \"%2.2f sec\" % period))\r\n",
    "\r\n",
    "            \r\n",
    "                                                                                            \r\n",
    "            # 简单的提前停止策略，认为连续达到某个准确率就可以停止了 \r\n",
    "            if acc1 >= good_acc1: \r\n",
    "                successive_count += 1 \r\n",
    "                logger.info(\"current acc1 {0} meets good {1}, successive count {2}\".format(acc1, good_acc1, successive_count)) \r\n",
    "                fluid.io.save_inference_model(dirname=train_parameters['save_freeze_dir'], \r\n",
    "                                              feeded_var_names=['img'], \r\n",
    "                                              target_vars=[out], \r\n",
    "                                              main_program=main_program, \r\n",
    "                                              executor=exe) \r\n",
    "                if successive_count >= successive_limit: \r\n",
    "                    logger.info(\"end training\") \r\n",
    "                    stop_train = True\r\n",
    "                    break \r\n",
    "            else: \r\n",
    "                successive_count = 0 \r\n",
    " \r\n",
    "            # 通用的保存策略，减小意外停止的损失 \r\n",
    "            if total_batch_count % sample_freq == 0: \r\n",
    "                logger.info(\"temp save {0} batch train result, current acc1 {1}\".format(total_batch_count, acc1)) \r\n",
    "                fluid.io.save_persistables(dirname=train_parameters['save_persistable_dir'], \r\n",
    "                                           main_program=main_program, \r\n",
    "                                           executor=exe) \r\n",
    "        #test\r\n",
    "        # acc_top1_test = fluid.layers.accuracy(input=out, label=label, k=1) \r\n",
    "        # exe_test = fluid.Executor(place) \r\n",
    "        # main_program_test = fluid.default_main_program() \r\n",
    "        # exe_test.run(fluid.default_startup_program()) \r\n",
    "        # test_fetch_list = [acc_top1_test.name] \r\n",
    "\r\n",
    "\r\n",
    "        test_fetch_list = [acc_top1.name] \r\n",
    "        for step_id, data in enumerate(test_batch_reader()): \r\n",
    "            t1_test = time.time()\r\n",
    "            acc_test = exe.run(program=test_program,\r\n",
    "                            feed=feeder.feed(data),\r\n",
    "                            fetch_list=test_fetch_list)\r\n",
    "            t2_test = time.time() \r\n",
    "            period_test = t2_test - t1_test\r\n",
    "            test_steps.append(test_step)\r\n",
    "            test_acc_scalar.add_record(test_step, acc_test[0])\r\n",
    "            all_test_acc.append(acc_test[0])\r\n",
    "            acc_test = np.mean(np.array(acc_test))\r\n",
    "            test_step += 1\r\n",
    "        logger.info(\"test result:Pass {0}, acc1 {1}, time {2}\".format(pass_id, acc_test,\"%2.2f sec\" % period_test))\r\n",
    "\r\n",
    "        # fluid.io.save_inference_model(dirname=train_parameters['save_test_dir'], \r\n",
    "        #                                       feeded_var_names=['img'], \r\n",
    "        #                                       target_vars=[out], \r\n",
    "        #                                       main_program=main_program, \r\n",
    "        #                                       executor=exe) \r\n",
    "        # test_acc = test_all()\r\n",
    "\r\n",
    "        if stop_train: \r\n",
    "            break \r\n",
    "    logger.info(\"training till last epcho, end training\") \r\n",
    "    fluid.io.save_persistables(dirname=train_parameters['save_persistable_dir'], \r\n",
    "                                           main_program=main_program, \r\n",
    "                                           executor=exe) \r\n",
    "    fluid.io.save_inference_model(dirname=train_parameters['save_freeze_dir'], \r\n",
    "                                              feeded_var_names=['img'], \r\n",
    "                                              target_vars=[out], \r\n",
    "                                              main_program=main_program, \r\n",
    "                                              executor=exe) \r\n",
    "                                              \r\n",
    "    # draw_train_process(\"training\",all_train_iters,all_train_costs,\"trainning cost\")\r\n",
    "    # draw_train_process(\"training\",all_train_iters,all_train_accs,\"trainning acc\")\r\n",
    "if __name__ == '__main__': \r\n",
    "    \r\n",
    "    #测试变量区\r\n",
    "    # data1 = []\r\n",
    "    # convdata = []\r\n",
    "    #paramdata=[]\r\n",
    "    #测试变量结束\r\n",
    "\r\n",
    "    #画图参数\r\n",
    "    \r\n",
    "    all_train_iters=[]\r\n",
    "    all_train_costs=[]\r\n",
    "    all_train_accs=[]\r\n",
    "    test_steps = []\r\n",
    "    all_test_acc = []\r\n",
    "\r\n",
    "    init_log_config()\r\n",
    "    init_train_parameters() \r\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # from matplotlib.font_manager import FontProperties\n",
    "# from PIL import ImageFont, ImageDraw, Image\n",
    "# fontpath = \"font/times.ttf\"\n",
    "# # font = ImageFont.truetype(fontpath, 32)\n",
    "\n",
    "\n",
    "\n",
    "# def draw_train_process(title,iters,y,label_y,var):\n",
    "#     # font = FontProperties(fname=r\"font/times.ttf\")\n",
    "#     # plt.rcParams['font.family'] = fname=r\"font/times.ttf\"\n",
    "#     # plt.rcParams['font.sans-serif'] = fname=r\"font/times.ttf\"\n",
    "#     # plt.rcParams['font.sans-serif'] = font1\n",
    "#     font1 = {'family': 'times',\n",
    "#         'weight': 'normal',\n",
    "#         'size': 16,\n",
    "#         }\n",
    "\n",
    "#     figure_dir = []\n",
    "#     fig = plt.figure(figsize=(10,5))\n",
    "#     plt.title(title, fontsize=30)\n",
    "#     plt.xlabel(\"iter\", fontsize=20)\n",
    "#     plt.ylabel(var, fontsize=20) \n",
    "#     plt.plot(iters, y,color='green',label=label_y) \n",
    "#     plt.legend()\n",
    "#     # plt.legend()\n",
    "#     # plt.grid()\n",
    "#     plt.rcParams['figure.dpi'] = 300 #分辨率\n",
    "#     plt.rcParams['xtick.direction'] = 'in' \n",
    "#     plt.rcParams['ytick.direction'] = 'in' \n",
    "#     figure_path = \"figure/\"\n",
    "#     figure_dir = figure_path + title + \".png\"\n",
    "#     print(figure_dir)\n",
    "#     plt.savefig(figure_dir)\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "# if __name__ == '__main__': \n",
    "#     draw_train_process(\"Training acc\",all_train_iters,all_train_accs,\"trainning acc\",\"acc\")\n",
    "#     draw_train_process(\"Training loss\",all_train_iters,all_train_costs,\"trainning loss\",\"loss\")\n",
    "#     draw_train_process(\"Test acc\",test_steps,all_test_acc,\"test acc\",\"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_iters = str(all_train_iters)\n",
    "# train_accs = str(all_train_accs)\n",
    "# train_costs = str(all_train_costs)\n",
    "\n",
    "# t_steps = str(test_steps)\n",
    "# test_acc = str(all_test_acc)\n",
    "# with open('训练结果存放.txt','w') as file_handle:   # .txt可以不自己新建,代码会自动新建\n",
    "#     for i in range(len(all_train_iters)):\n",
    "#         file_handle.write(train_iters[i]+'\\t'+train_accs[i]+'\\t'+train_costs+'\\n')\n",
    "# file_handle.close()\n",
    "# with open('测试结果存放.txt','w') as file_handle:   # .txt可以不自己新建,代码会自动新建\n",
    "#     for i in range(len(t_steps)):\n",
    "#         file_handle.write(t_steps[i]+'\\t'+test_acc[i]+'\\n')\n",
    "# file_handle.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iters     accs     costs\n",
      "0      1  0.15625  2.908285\n",
      "1      2  0.21875  2.562631\n",
      "2      3  0.21875  2.678508\n",
      "3      4  0.00000  3.664262\n",
      "4      5  0.12500  2.925835\n",
      "   steps      acc\n",
      "0      0  0.28125\n",
      "1      1  0.34375\n",
      "2      2  0.18750\n",
      "3      3  0.18750\n",
      "4      4  0.18750\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "name = [\"iters\"]\n",
    "train1=pd.DataFrame(columns=name,data=all_train_iters)\n",
    "name = [\"accs\"]\n",
    "train2=pd.DataFrame(columns=name,data=all_train_accs)\n",
    "name = [\"costs\"]\n",
    "train3=pd.DataFrame(columns=name,data=all_train_costs)\n",
    "train = pd.concat([train1,train2,train3],axis=1,ignore_index=False)\n",
    "print(train[0:5])\n",
    "train.to_csv('train data.csv',encoding='gbk')\n",
    "\n",
    "name = [\"steps\"]\n",
    "test1=pd.DataFrame(columns=name,data=test_steps)\n",
    "name = [\"acc\"]\n",
    "test2=pd.DataFrame(columns=name,data=all_test_acc)\n",
    "test = pd.concat([test1,test2],axis=1,ignore_index=False)\n",
    "print(test[0:5])\n",
    "test.to_csv('test data.csv',encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##输出卷积层图像\n",
    "# for i in range(1,3):\n",
    "#     fig = plt.figure(figsize=(5,5))\n",
    "#     ax1 = fig.add_subplot(2,2,1)\n",
    "#     plt.imshow(data1[i])\n",
    "#     ax1 = fig.add_subplot(2,2,2)\n",
    "#     plt.imshow(convdata[i])\n",
    "# def draw_image_1(n):\n",
    "#     plt.imshow(data1[n])\n",
    "#     plt.xticks([])  #去掉横坐标值\n",
    "#     plt.yticks([])  #去掉纵坐标值\n",
    "#     j = 0\n",
    "#     fig = plt.figure(figsize=(20,10))\n",
    "#     for i in range(1,33):\n",
    "#         ax1 = fig.add_subplot(4,8,i)\n",
    "#         plt.imshow(convdata[n][0][j])\n",
    "#         plt.xticks([])  #去掉横坐标值\n",
    "#         plt.yticks([])  #去掉纵坐标值\n",
    "#         j += 1\n",
    "\n",
    "# if __name__ == '__main__': \n",
    "#     draw_image_1(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def draw_image_1(n):\n",
    "#     plt.imshow(data1[n])\n",
    "#     plt.xticks([])  #去掉横坐标值\n",
    "#     plt.yticks([])  #去掉纵坐标值\n",
    "#     j = 0\n",
    "#     # fig = plt.figure(figsize=(6,6))\n",
    "#     # for i in range(1,33):\n",
    "#     #     ax1 = fig.add_subplot(6,6,i)\n",
    "#     #     plt.imshow(convdata_MN[n][0][j])\n",
    "#     #     plt.xticks([])  #去掉横坐标值\n",
    "#     #     plt.yticks([])  #去掉纵坐标值\n",
    "#     #     j += 1\n",
    "\n",
    "# if __name__ == '__main__': \n",
    "#     draw_image_1(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def rgb2hsv(img):\n",
    "#     h = img.shape[0]\n",
    "#     w = img.shape[1]\n",
    "#     H = np.zeros((h,w),np.float32)\n",
    "#     S = np.zeros((h, w), np.float32)\n",
    "#     V = np.zeros((h, w), np.float32)\n",
    "#     r,g,b = cv2.split(img)\n",
    "#     r, g, b = r/255.0, g/255.0, b/255.0\n",
    "#     for i in range(0, h):\n",
    "#         for j in range(0, w):\n",
    "#             mx = max((b[i, j], g[i, j], r[i, j]))\n",
    "#             mn = min((b[i, j], g[i, j], r[i, j]))\n",
    "#             dt=mx-mn\n",
    "\n",
    "#             if mx == mn:\n",
    "#                 H[i, j] = 0\n",
    "#             elif mx == r[i, j]:\n",
    "#                 if g[i, j] >= b[i, j]:\n",
    "#                     H[i, j] = (60 * ((g[i, j]) - b[i, j]) / dt)\n",
    "#                 else:\n",
    "#                     H[i, j] = (60 * ((g[i, j]) - b[i, j]) / dt)+360\n",
    "#             elif mx == g[i, j]:\n",
    "#                 H[i, j] = 60 * ((b[i, j]) - r[i, j]) / dt + 120\n",
    "#             elif mx == b[i, j]:\n",
    "#                 H[i, j] = 60 * ((r[i, j]) - g[i, j]) / dt+ 240\n",
    "#             H[i,j] =int( H[i,j] / 2)\n",
    "\n",
    "#             #S\n",
    "#             if mx == 0:\n",
    "#                 S[i, j] = 0\n",
    "#             else:\n",
    "#                 S[i, j] =int( dt/mx*255)\n",
    "#             #V\n",
    "#             V[i, j] =int( mx*255)\n",
    "\n",
    "#     return H, S, V\n",
    "\n",
    "# path_image = (\"WechatIMG1332.png\")\n",
    "# img = Image.open(path_image)\n",
    "# #img = np.array(img).astype('float32')\n",
    "# plt.imshow(img)\n",
    "\n",
    "# # plt.xticks([])  #去掉横坐标值\n",
    "# # plt.yticks([])  #去掉纵坐标值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total eval count:738 cost time:75.68 sec predict accuracy:0.29132791327913277\n"
     ]
    }
   ],
   "source": [
    "#top-1\n",
    "from __future__ import absolute_import     \n",
    "from __future__ import division     \n",
    "from __future__ import print_function     \n",
    "     \n",
    "import os     \n",
    "import numpy as np     \n",
    "import random     \n",
    "import time     \n",
    "import codecs     \n",
    "import sys     \n",
    "import functools     \n",
    "import math     \n",
    "import paddle     \n",
    "import paddle.fluid as fluid     \n",
    "from paddle.fluid import core     \n",
    "from paddle.fluid.param_attr import ParamAttr     \n",
    "from PIL import Image, ImageEnhance     \n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True    \n",
    "\n",
    "target_size = [3, 224, 224]     \n",
    "mean_rgb = [127.5, 127.5, 127.5]     \n",
    "data_dir = path     \n",
    "eval_file = \"eval.txt\"     \n",
    "use_gpu = True     \n",
    "place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()     \n",
    "exe = fluid.Executor(place)     \n",
    "save_freeze_dir = \"./freeze-model\"     \n",
    "[inference_program, feed_target_names, fetch_targets] = fluid.io.load_inference_model(dirname=save_freeze_dir, executor=exe)     \n",
    "# print(fetch_targets)     \n",
    "     \n",
    "     \n",
    "def crop_image(img, target_size):     \n",
    "    width, height = img.size\n",
    "    p = min(target_size[2] / width, target_size[1] / height)\n",
    "    resized_h = int(height * p)\n",
    "    resized_w = int(width * p)\n",
    "    img = img.resize((resized_w, resized_h), Image.BILINEAR)\n",
    "    w_start = (resized_w - target_size[2]) / 2     \n",
    "    h_start = (resized_h - target_size[1]) / 2     \n",
    "    w_end = w_start + target_size[2]     \n",
    "    h_end = h_start + target_size[1]     \n",
    "    img = img.crop((w_start, h_start, w_end, h_end))     \n",
    "    return img     \n",
    "\n",
    "\n",
    "def resize_img(img, target_size):     \n",
    "    ret = img.resize((target_size[1], target_size[2]), Image.BILINEAR)     \n",
    "    return ret     \n",
    "\n",
    "\n",
    "def read_image(img_path):     \n",
    "    img = Image.open(img_path)     \n",
    "    if img.mode != 'RGB':     \n",
    "        img = img.convert('RGB')     \n",
    "    # img = crop_image(img, target_size)\n",
    "    img = resize_img(img, target_size)\n",
    "    img = np.array(img).astype('float32')     \n",
    "    img -= mean_rgb     \n",
    "    img = img.transpose((2, 0, 1))  # HWC to CHW     \n",
    "    img *= 0.007843     \n",
    "    img = img[np.newaxis,:]     \n",
    "    return img     \n",
    "     \n",
    "     \n",
    "def infer(image_path):     \n",
    "    tensor_img = read_image(image_path)     \n",
    "    label = exe.run(inference_program, feed={feed_target_names[0]: tensor_img}, fetch_list=fetch_targets)     \n",
    "    return np.argmax(label)     \n",
    "     \n",
    "     \n",
    "def eval_all():     \n",
    "    eval_file_path = os.path.join(data_dir, eval_file)     \n",
    "    total_count = 0     \n",
    "    right_count = 0     \n",
    "    with codecs.open(eval_file_path, encoding='utf-8') as flist:      \n",
    "        lines = [line.strip() for line in flist]     \n",
    "        t1 = time.time()     \n",
    "        for line in lines:     \n",
    "            total_count += 1     \n",
    "            parts = line.strip().split()     \n",
    "            result = infer(parts[0])     \n",
    "            # print(\"infer result:{0} answer:{1}\".format(result, parts[1]))     \n",
    "            if str(result) == parts[1]:     \n",
    "                right_count += 1     \n",
    "        period = time.time() - t1     \n",
    "        print(\"total eval count:{0} cost time:{1} predict accuracy:{2}\".format(total_count, \"%2.2f sec\" % period, right_count / total_count))     \n",
    "     \n",
    "     \n",
    "if __name__ == '__main__':     \n",
    "    eval_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total eval count:738 cost time:82.45 sec predict accuracy:0.7005420054200542\n"
     ]
    }
   ],
   "source": [
    "# top-5acc \n",
    "from __future__ import absolute_import     \n",
    "from __future__ import division     \n",
    "from __future__ import print_function     \n",
    "     \n",
    "import os     \n",
    "import numpy as np     \n",
    "import random     \n",
    "import time     \n",
    "import codecs     \n",
    "import sys     \n",
    "import functools     \n",
    "import math     \n",
    "import paddle     \n",
    "import paddle.fluid as fluid     \n",
    "from paddle.fluid import core     \n",
    "from paddle.fluid.param_attr import ParamAttr     \n",
    "from PIL import Image, ImageEnhance   \n",
    "import heapq  \n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "     \n",
    "# path = 'data/data21566/images-2.17_1'   \n",
    "# path = 'data/data9048/images-6.4' \n",
    "target_size = [3, 224, 224]     \n",
    "mean_rgb = [127.5, 127.5, 127.5]     \n",
    "data_dir = path     \n",
    "eval_file = \"eval.txt\"     \n",
    "use_gpu = True     \n",
    "place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()     \n",
    "exe = fluid.Executor(place)     \n",
    "save_freeze_dir = \"./freeze-model\"     \n",
    "[inference_program, feed_target_names, fetch_targets] = fluid.io.load_inference_model(dirname=save_freeze_dir, executor=exe)     \n",
    "# print(fetch_targets)     \n",
    "     \n",
    "     \n",
    "def crop_image(img, target_size):     \n",
    "    width, height = img.size\n",
    "    p = min(target_size[2] / width, target_size[1] / height)\n",
    "    resized_h = int(height * p)\n",
    "    resized_w = int(width * p)\n",
    "    img = img.resize((resized_w, resized_h), Image.BILINEAR)\n",
    "    w_start = (resized_w - target_size[2]) / 2     \n",
    "    h_start = (resized_h - target_size[1]) / 2     \n",
    "    w_end = w_start + target_size[2]     \n",
    "    h_end = h_start + target_size[1]     \n",
    "    img = img.crop((w_start, h_start, w_end, h_end))     \n",
    "    return img     \n",
    "\n",
    "\n",
    "def resize_img(img, target_size):     \n",
    "    ret = img.resize((target_size[1], target_size[2]), Image.BILINEAR)     \n",
    "    return ret     \n",
    "\n",
    "\n",
    "def read_image(img_path):     \n",
    "    img = Image.open(img_path)     \n",
    "    if img.mode != 'RGB':     \n",
    "        img = img.convert('RGB')     \n",
    "    # img = crop_image(img, target_size)\n",
    "    img = resize_img(img, target_size)\n",
    "    img = np.array(img).astype('float32')     \n",
    "    img -= mean_rgb     \n",
    "    img = img.transpose((2, 0, 1))  # HWC to CHW     \n",
    "    img *= 0.007843     \n",
    "    img = img[np.newaxis,:]     \n",
    "    return img     \n",
    "     \n",
    "     \n",
    "def infer(image_path):     \n",
    "    tensor_img = read_image(image_path)     \n",
    "    label = exe.run(inference_program, feed={feed_target_names[0]: tensor_img}, fetch_list=fetch_targets)  \n",
    "    # re1 = heapq.nlargest(3, label) #求最大的三个元素，并排序\n",
    "    label = label[0]\n",
    "    label = label.tolist()\n",
    "    label = label[0]\n",
    "    # print(label[0])\n",
    "    re2 = map(label.index, heapq.nlargest(5, label)) #求最大的三个索引 \n",
    "    # print(list(re2)) \n",
    "   \n",
    "    return re2\n",
    "     \n",
    "     \n",
    "def eval_all():     \n",
    "    eval_file_path = os.path.join(data_dir, eval_file)     \n",
    "    total_count = 0     \n",
    "    right_count = 0     \n",
    "    with codecs.open(eval_file_path, encoding='utf-8') as flist:      \n",
    "        lines = [line.strip() for line in flist]     \n",
    "        t1 = time.time()     \n",
    "        for line in lines:     \n",
    "            total_count += 1     \n",
    "            parts = line.strip().split()     \n",
    "            result = infer(parts[0])     \n",
    "            # print(\"infer result:{0} answer:{1}\".format(result, parts[1]))\n",
    "            # print(parts)\n",
    "            for re in result:    \n",
    "                if str(re) == parts[1]:     \n",
    "                    right_count += 1     \n",
    "        period = time.time() - t1     \n",
    "        print(\"total eval count:{0} cost time:{1} predict accuracy:{2}\".format(total_count, \"%2.2f sec\" % period, right_count / total_count))     \n",
    "     \n",
    "     \n",
    "if __name__ == '__main__':     \n",
    "    \n",
    "    eval_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
